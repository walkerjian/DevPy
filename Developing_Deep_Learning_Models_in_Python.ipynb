{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPkxEkOQOBg3Uhzupt18GQH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/walkerjian/DevPy/blob/main/Developing_Deep_Learning_Models_in_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 1: Overview of Some Deep Learning Libraries\n",
        "\n",
        "Deep learning, a subset of machine learning, employs neural networks with many layers (hence \"deep\") to analyze various factors of data. These deep neural networks attempt to simulate the behavior of the human brain—albeit far from matching its complexity—allowing it to \"learn\" from large amounts of data. While a neural network with a single layer can make approximate predictions, additional hidden layers can help refine accuracy.\n",
        "\n",
        "With the rise in the importance of deep learning in a range of applications from image and speech recognition to medical diagnosis and autonomous vehicles, various libraries and frameworks have been developed to facilitate the design, training, and implementation of deep neural networks. In this chapter, we will provide an overview of some of the most popular deep learning libraries.\n",
        "\n",
        "## 1.1 TensorFlow\n",
        "\n",
        "**Developed by:** Google Brain  \n",
        "**First Released:** 2015\n",
        "\n",
        "**Overview:**  \n",
        "TensorFlow is an open-source software library for numerical computation using data flow graphs. The graph nodes represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) that flow between them.\n",
        "\n",
        "**Features:**\n",
        "1. Supports both CPU and GPU computing devices.\n",
        "2. Extensive documentation and community support.\n",
        "3. Flexibility to develop on multiple platforms like mobile and browsers.\n",
        "4. TensorFlow Extended (TFX) for deploying machine learning pipelines.\n",
        "\n",
        "**Python Example:**\n",
        "Here's a simple example of defining and computing a tensor in TensorFlow:\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define a constant tensor\n",
        "hello = tf.constant('Hello, TensorFlow!')\n",
        "\n",
        "# Start a TF session\n",
        "with tf.Session() as sess:\n",
        "    print(sess.run(hello))\n",
        "```\n",
        "\n",
        "## 1.2 PyTorch\n",
        "\n",
        "**Developed by:** Facebook's AI Research lab  \n",
        "**First Released:** 2016\n",
        "\n",
        "**Overview:**  \n",
        "PyTorch is an open-source deep learning platform that provides a flexible and dynamic computational graph, which makes it particularly suited to research.\n",
        "\n",
        "**Features:**\n",
        "1. Dynamic computation graph, which is beneficial for dynamic input/output or recurrent neural networks.\n",
        "2. Native support for Python and uses Python's native debugging tools.\n",
        "3. Extensive libraries and tools, such as TorchVision for computer vision tasks.\n",
        "4. TorchServe for model serving.\n",
        "\n",
        "**Python Example:**  \n",
        "Here's a simple example of creating a tensor in PyTorch:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "# Create a tensor\n",
        "x = torch.Tensor([1, 2, 3, 4])\n",
        "print(x)\n",
        "```\n",
        "\n",
        "## 1.3 Keras\n",
        "\n",
        "**Developed by:** François Chollet  \n",
        "**First Released:** 2015\n",
        "\n",
        "**Overview:**  \n",
        "Keras is an open-source neural network library written in Python. It's capable of running on top of TensorFlow, Microsoft Cognitive Toolkit, Theano, or PlaidML. Designed to enable fast experimentation with deep neural networks, it focuses on being user-friendly, modular, and extensible.\n",
        "\n",
        "**Features:**\n",
        "1. Simple and intuitive API, making it easy for beginners.\n",
        "2. Supports multiple backends.\n",
        "3. Modular architecture where you can define, save, load, and reuse models.\n",
        "4. Integrated with lower-level deep learning languages, primarily TensorFlow.\n",
        "\n",
        "**Python Example:**  \n",
        "Here's a simple example of creating a sequential model in Keras:\n",
        "\n",
        "```python\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# Create a sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Add layers to the model\n",
        "model.add(Dense(units=64, activation='relu', input_dim=100))\n",
        "model.add(Dense(units=10, activation='softmax'))\n",
        "```\n",
        "\n",
        "## 1.4 Conclusion\n",
        "\n",
        "These are just a few of the myriad deep learning libraries available today. Each has its strengths, trade-offs, and areas of application. The best library largely depends on the specific requirements of the project and the preference of the developer or researcher. As the deep learning field evolves, it's crucial to stay updated with the latest developments in these libraries and watch out for emerging tools that might offer new capabilities or efficiencies."
      ],
      "metadata": {
        "id": "fZ_fKNNsT77I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 2: Introduction to PyTorch\n",
        "\n",
        "PyTorch, developed by Facebook's AI Research lab, is one of the leading deep learning frameworks. Known for its dynamic computational graph, which contrasts with TensorFlow's static graph, PyTorch has gained immense popularity, especially in the research community. This chapter provides an introduction to the core concepts and functionalities of PyTorch.\n",
        "\n",
        "## 2.1 What is PyTorch?\n",
        "\n",
        "At its core, PyTorch is a library that provides multidimensional arrays, called tensors, and an assortment of mathematical operations to manipulate these tensors. Furthermore, it offers a dynamic computation graph, which means that the graph is built on-the-fly as operations are created. This property makes PyTorch particularly suitable for tasks that require dynamic network architectures, such as certain recurrent neural networks.\n",
        "\n",
        "## 2.2 Tensors: The Building Blocks\n",
        "\n",
        "Tensors are the fundamental data structures in PyTorch, analogous to arrays in Python or matrices in mathematics. They can be used to encode inputs, outputs, and parameters of neural networks.\n",
        "\n",
        "**Python Example:**  \n",
        "Creating a tensor in PyTorch:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "# Create a 1D tensor\n",
        "tensor_1d = torch.tensor([1, 2, 3, 4])\n",
        "print(tensor_1d)\n",
        "\n",
        "# Create a 2D tensor\n",
        "tensor_2d = torch.tensor([[1, 2], [3, 4]])\n",
        "print(tensor_2d)\n",
        "```\n",
        "\n",
        "## 2.3 Autograd: Automatic Differentiation\n",
        "\n",
        "One of PyTorch's most powerful features is its `autograd` package, which provides automatic differentiation for all operations on tensors. This capability is critical for training neural networks, where gradients are required for optimization.\n",
        "\n",
        "In PyTorch, if a tensor's `.requires_grad` attribute is set to `True`, it will start to track all operations on it. When finished with the computations, you can call `.backward()` to compute all gradients automatically.\n",
        "\n",
        "**Python Example:**  \n",
        "Using `autograd` to compute gradients:\n",
        "\n",
        "```python\n",
        "# Create a tensor and set requires_grad=True to track computation with it\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "\n",
        "# Define a simple computation\n",
        "y = x**2\n",
        "\n",
        "# Compute gradients\n",
        "y.backward()\n",
        "\n",
        "# Print the gradient\n",
        "print(x.grad)  # dy/dx = 2*x = 4.0\n",
        "```\n",
        "\n",
        "## 2.4 Neural Networks with PyTorch\n",
        "\n",
        "PyTorch's `nn` module provides the necessary building blocks to construct neural networks. The `nn.Module` is the base class for all neural network modules, and custom models should also subclass this class.\n",
        "\n",
        "**Python Example:**  \n",
        "Defining a simple feed-forward neural network:\n",
        "\n",
        "```python\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "```\n",
        "\n",
        "## 2.5 Training a Model\n",
        "\n",
        "Training a neural network typically involves the following steps:\n",
        "1. Define a neural network model.\n",
        "2. Choose a loss function.\n",
        "3. Choose an optimization method.\n",
        "4. Feed data into the model in batches.\n",
        "5. Compute the loss.\n",
        "6. Backpropagate to compute gradients.\n",
        "7. Update the model's weights.\n",
        "\n",
        "PyTorch provides utilities for all these steps, making the training process efficient and relatively straightforward.\n",
        "\n",
        "## 2.6 Conclusion\n",
        "\n",
        "PyTorch offers a blend of flexibility and power that makes it an ideal tool for both beginners and researchers in the deep learning domain. Its dynamic computation graph, rich tensor operations, automatic differentiation capabilities, and high-level modules for neural network constructions are the primary reasons behind its burgeoning popularity. Whether you're looking to implement a complex neural network model or merely understand the mechanics behind them, PyTorch is a robust tool to have in your arsenal."
      ],
      "metadata": {
        "id": "jn__QWnkUopU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 3: Manipulating Tensors in PyTorch\n",
        "\n",
        "In the realm of deep learning, data representation plays a pivotal role. PyTorch provides tensors as its primary data structure to encapsulate this data. Tensors in PyTorch can be thought of as generalizations of matrices to higher-dimensional spaces. This chapter delves into the various operations and manipulations you can perform on tensors using PyTorch.\n",
        "\n",
        "## 3.1 Creating Tensors\n",
        "\n",
        "Before diving into tensor manipulations, let's understand how to create them:\n",
        "\n",
        "**From a List or Array:**  \n",
        "You can create a tensor directly from a Python list or a NumPy array.\n",
        "\n",
        "```python\n",
        "# Creating tensor from a list\n",
        "tensor_from_list = torch.tensor([1, 2, 3, 4])\n",
        "\n",
        "# Creating tensor from a numpy array\n",
        "import numpy as np\n",
        "array = np.array([1, 2, 3, 4])\n",
        "tensor_from_array = torch.from_numpy(array)\n",
        "```\n",
        "\n",
        "**Using Built-in Functions:**  \n",
        "PyTorch provides several built-in functions to generate tensors.\n",
        "\n",
        "```python\n",
        "# Zeros tensor of shape (3, 3)\n",
        "zeros_tensor = torch.zeros(3, 3)\n",
        "\n",
        "# Ones tensor of shape (2, 4)\n",
        "ones_tensor = torch.ones(2, 4)\n",
        "\n",
        "# Tensor with random values of shape (3, 3)\n",
        "random_tensor = torch.rand(3, 3)\n",
        "```\n",
        "\n",
        "## 3.2 Indexing and Slicing\n",
        "\n",
        "Much like Python lists or NumPy arrays, you can index and slice tensors to extract or modify specific elements or sub-tensors.\n",
        "\n",
        "```python\n",
        "tensor = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "\n",
        "# Get element at row 1, column 2\n",
        "element = tensor[1, 2]  # Outputs 6\n",
        "\n",
        "# Get the first row\n",
        "first_row = tensor[0, :]  # Outputs [1, 2, 3]\n",
        "```\n",
        "\n",
        "## 3.3 Reshaping Tensors\n",
        "\n",
        "Reshaping tensors is a common operation, especially when preparing data for neural network layers.\n",
        "\n",
        "```python\n",
        "tensor = torch.tensor([1, 2, 3, 4, 5, 6])\n",
        "\n",
        "# Reshape to a 3x2 tensor\n",
        "reshaped_tensor = tensor.view(3, 2)\n",
        "\n",
        "# Flatten a tensor\n",
        "flattened_tensor = tensor.view(-1)  # The -1 infers the necessary size\n",
        "```\n",
        "\n",
        "## 3.4 Arithmetic Operations\n",
        "\n",
        "You can perform element-wise arithmetic operations on tensors.\n",
        "\n",
        "```python\n",
        "tensor_a = torch.tensor([1, 2, 3])\n",
        "tensor_b = torch.tensor([4, 5, 6])\n",
        "\n",
        "# Addition\n",
        "sum_tensors = tensor_a + tensor_b\n",
        "\n",
        "# Multiplication\n",
        "product_tensors = tensor_a * tensor_b\n",
        "```\n",
        "\n",
        "For matrix multiplication, use the `matmul` function:\n",
        "\n",
        "```python\n",
        "matrix_a = torch.tensor([[1, 2], [3, 4]])\n",
        "matrix_b = torch.tensor([[2, 0], [0, 2]])\n",
        "result = torch.matmul(matrix_a, matrix_b)\n",
        "```\n",
        "\n",
        "## 3.5 Broadcasting\n",
        "\n",
        "Broadcasting is a powerful mechanism that allows PyTorch to perform arithmetic operations on tensors of different shapes. It automatically expands the dimensions of the smaller tensor to match the shape of the larger tensor.\n",
        "\n",
        "```python\n",
        "tensor = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "scalar = 2\n",
        "\n",
        "# Broadcasting scalar to tensor's shape and multiplying\n",
        "result_tensor = tensor * scalar\n",
        "```\n",
        "\n",
        "## 3.6 In-place Operations\n",
        "\n",
        "Operations that store the result into the operand are called in-place. They are denoted by a `_` suffix in PyTorch.\n",
        "\n",
        "```python\n",
        "tensor = torch.tensor([1, 2, 3])\n",
        "tensor.add_(5)  # Adds 5 to each element in-place\n",
        "```\n",
        "\n",
        "## 3.7 Conclusion\n",
        "\n",
        "Manipulating tensors is fundamental when working with PyTorch. Whether it's reshaping tensors to fit layers of a neural network, slicing tensors to extract features, or performing arithmetic operations to transform data, a solid grasp of tensor operations is crucial for any deep learning practitioner using PyTorch. The operations introduced in this chapter are just the tip of the iceberg, and there's a vast array of functions and methods available in PyTorch to aid in tensor manipulation and mathematical computations."
      ],
      "metadata": {
        "id": "UNXQ6UahVAcF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 4: Using Autograd in PyTorch to Solve a Regression Problem\n",
        "\n",
        "PyTorch's `autograd` package is the cornerstone that enables automatic differentiation, a fundamental capability for training machine learning models. It allows developers to automatically compute gradients, which are then used to update model parameters. This chapter will guide you through using `autograd` to solve a simple regression problem.\n",
        "\n",
        "## 4.1 What is Regression?\n",
        "\n",
        "Regression aims to model and analyze the relationships between variables. In simple terms, given an input variable \\( X \\) and an output variable \\( Y \\), regression tries to find a function that maps \\( X \\) to \\( Y \\). For linear regression, this function is a linear equation.\n",
        "\n",
        "## 4.2 Problem Statement\n",
        "\n",
        "Consider a dataset where the relationship between \\( X \\) and \\( Y \\) is approximately linear. Our goal is to fit a line to this data that minimizes the distance (or error) between the line and the actual data points.\n",
        "\n",
        "Mathematically, we aim to find weights \\( w \\) and bias \\( b \\) such that:\n",
        "\n",
        "\\[ Y_{\\text{pred}} = wX + b \\]\n",
        "\n",
        "Here, \\( Y_{\\text{pred}} \\) is the predicted output.\n",
        "\n",
        "## 4.3 Building the Regression Model\n",
        "\n",
        "Let's start by defining our regression model using PyTorch's `nn.Module`.\n",
        "\n",
        "```python\n",
        "import torch.nn as nn\n",
        "\n",
        "class LinearRegression(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(LinearRegression, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, 1)  # input_dim-dimensional input to a 1-dimensional output\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "```\n",
        "\n",
        "## 4.4 Loss Function\n",
        "\n",
        "For regression problems, the Mean Squared Error (MSE) is a common choice. It measures the average squared difference between the estimated values and the actual value.\n",
        "\n",
        "\\[ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (Y_i - Y_{\\text{pred}_i})^2 \\]\n",
        "\n",
        "Where:\n",
        "- \\( n \\) is the number of data points.\n",
        "- \\( Y_i \\) is the actual value.\n",
        "- \\( Y_{\\text{pred}_i} \\) is the predicted value.\n",
        "\n",
        "In PyTorch, we can use the `MSELoss` function from the `nn` module.\n",
        "\n",
        "## 4.5 Training the Model\n",
        "\n",
        "Training involves:\n",
        "1. Forward pass: Compute the predicted output with the current weights.\n",
        "2. Compute the loss.\n",
        "3. Backward pass: Use `autograd` to compute the gradient of the loss with respect to model parameters.\n",
        "4. Update the weights using an optimization algorithm, e.g., Stochastic Gradient Descent (SGD).\n",
        "\n",
        "Here's a simplified training loop:\n",
        "\n",
        "```python\n",
        "import torch.optim as optim\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.01\n",
        "epochs = 100\n",
        "\n",
        "# Model, Loss, and Optimizer\n",
        "model = LinearRegression(input_dim=1)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Sample data\n",
        "X_train = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
        "Y_train = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    outputs = model(X_train)\n",
        "    loss = criterion(outputs, Y_train)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()  # Clear existing gradients\n",
        "    loss.backward()        # Compute gradients\n",
        "    optimizer.step()       # Update weights\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "```\n",
        "\n",
        "## 4.6 Evaluating the Model\n",
        "\n",
        "Once the model is trained, you can use it to make predictions:\n",
        "\n",
        "```python\n",
        "# Predicting\n",
        "X_test = torch.tensor([[5]], dtype=torch.float32)\n",
        "Y_pred = model(X_test)\n",
        "print(f\"Prediction for input {X_test.item()}: {Y_pred.item()}\")\n",
        "```\n",
        "\n",
        "## 4.7 Conclusion\n",
        "\n",
        "Using `autograd` in PyTorch simplifies the process of computing gradients and updating model parameters, making the training of machine learning models more accessible. By understanding the underlying concepts and mechanics of `autograd`, one can harness the power of PyTorch to tackle more complex problems and dive deeper into the world of deep learning."
      ],
      "metadata": {
        "id": "iUO-hv42VVUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 5: A Crash Course on Deep Learning\n",
        "\n",
        "Deep learning, a subfield of machine learning, has gained monumental traction over the past decade, pushing the boundaries of what machines can perceive, understand, and generate. This chapter offers a concise introduction to deep learning, its core concepts, and its transformative applications.\n",
        "\n",
        "## 5.1 What is Deep Learning?\n",
        "\n",
        "Deep learning is a subset of machine learning that employs neural networks with many layers (hence \"deep\") to analyze various factors of data. These deep neural networks attempt to simulate the behavior of the human brain, allowing it to \"learn\" from large amounts of data.\n",
        "\n",
        "## 5.2 Neural Networks\n",
        "\n",
        "At the heart of deep learning lies the concept of artificial neural networks. These networks are inspired by the structure of the human brain, consisting of interconnected nodes (or \"neurons\").\n",
        "\n",
        "### 5.2.1 Layers in a Neural Network\n",
        "\n",
        "- **Input Layer:** Represents the input features.\n",
        "- **Hidden Layers:** Layers between the input and output, where the actual processing happens. Deep networks have multiple hidden layers, which is where the term \"deep\" learning originates.\n",
        "- **Output Layer:** Produces the final prediction or classification.\n",
        "\n",
        "### 5.2.2 Activation Functions\n",
        "\n",
        "Activation functions introduce non-linearity into the network, enabling it to learn complex patterns. Common activation functions include the ReLU (Rectified Linear Unit), sigmoid, and tanh.\n",
        "\n",
        "## 5.3 Training Neural Networks\n",
        "\n",
        "Training a neural network involves feeding it data and adjusting its weights based on the prediction errors.\n",
        "\n",
        "1. **Feedforward:** Calculate the predicted output given the current weights and the input data.\n",
        "2. **Loss Calculation:** Compute the difference between the predicted output and the actual target values.\n",
        "3. **Backpropagation:** Adjust the weights of the network in a manner that minimizes the loss.\n",
        "4. **Optimization:** Update the weights using optimization algorithms like Gradient Descent.\n",
        "\n",
        "## 5.4 Deep Learning Architectures\n",
        "\n",
        "There are various specialized neural network architectures, each designed for specific types of tasks:\n",
        "\n",
        "- **Convolutional Neural Networks (CNNs):** Ideal for image data. They use convolutional layers to filter input data for useful information.\n",
        "- **Recurrent Neural Networks (RNNs):** Suitable for sequential data like time series or natural language. They have loops to allow information persistence.\n",
        "- **Transformers:** A newer architecture that's become the standard for many NLP (Natural Language Processing) tasks.\n",
        "- **Autoencoders:** Used for unsupervised learning tasks, especially for data compression and noise reduction.\n",
        "- **Generative Adversarial Networks (GANs):** Consist of two networks, a generator, and a discriminator, and are used for generating new data that is similar to the input data.\n",
        "\n",
        "## 5.5 Applications of Deep Learning\n",
        "\n",
        "Deep learning has a wide range of applications:\n",
        "\n",
        "1. **Image and Video Analysis:** Image classification, object detection, facial recognition, and video analysis.\n",
        "2. **Natural Language Processing:** Sentiment analysis, machine translation, and chatbots.\n",
        "3. **Voice and Audio Processing:** Voice assistants, voice recognition, and music generation.\n",
        "4. **Medical Diagnosis:** Analyzing medical images, predicting diseases, and personalizing patient treatment plans.\n",
        "5. **Autonomous Vehicles:** Enabling cars to perceive their environment and make driving decisions.\n",
        "\n",
        "## 5.6 Challenges in Deep Learning\n",
        "\n",
        "While deep learning offers powerful capabilities, it comes with challenges:\n",
        "\n",
        "- **Data Needs:** Deep learning models often require vast amounts of data.\n",
        "- **Computational Resources:** Training can be resource-intensive and time-consuming.\n",
        "- **Interpretability:** Deep learning models, particularly complex ones, can act as \"black boxes,\" making it challenging to understand their decisions.\n",
        "- **Overfitting:** Without proper precautions, models can become too tailored to the training data and perform poorly on new, unseen data.\n",
        "\n",
        "## 5.7 Conclusion\n",
        "\n",
        "Deep learning is reshaping the landscape of technology and research, driving innovations and enhancements across various domains. As computational power increases and algorithms become more refined, the capabilities and applications of deep learning will continue to expand, bridging the gap between machines and human-like intelligence. Whether you're a budding data scientist, a seasoned researcher, or a curious enthusiast, understanding the fundamentals of deep learning is crucial in this rapidly evolving digital age."
      ],
      "metadata": {
        "id": "eHQcuorcVstq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 6: Multilayer Perceptron Building Blocks in PyTorch\n",
        "\n",
        "The Multilayer Perceptron (MLP), also known as a feedforward neural network, is one of the simplest and most foundational deep learning models. Despite its simplicity, the MLP can approximate virtually any function, given sufficient data and computational power. In this chapter, we'll explore the building blocks of an MLP using PyTorch.\n",
        "\n",
        "## 6.1 Basic Structure of an MLP\n",
        "\n",
        "An MLP comprises three main types of layers:\n",
        "\n",
        "1. **Input Layer:** Represents the features of the dataset.\n",
        "2. **Hidden Layers:** One or more layers where the actual computation happens. Each layer contains a set of neurons (or nodes).\n",
        "3. **Output Layer:** Produces the final predictions or classifications.\n",
        "\n",
        "Data flows from the input layer through the hidden layers and finally to the output layer in a feedforward manner.\n",
        "\n",
        "## 6.2 Building an MLP in PyTorch\n",
        "\n",
        "### 6.2.1 Defining the Model\n",
        "\n",
        "Using the `nn.Module` class in PyTorch, we can define an MLP:\n",
        "\n",
        "```python\n",
        "import torch.nn as nn\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "```\n",
        "\n",
        "In this example, we have an input layer, one hidden layer, and an output layer. The ReLU activation function is applied after the hidden layer.\n",
        "\n",
        "### 6.2.2 Activation Functions\n",
        "\n",
        "Activation functions introduce non-linearities into the network. Some popular activation functions include:\n",
        "\n",
        "- **ReLU (Rectified Linear Unit):** A simple function that returns the input for positive values and zero for negative values.\n",
        "- **Sigmoid:** Maps input values to the range (0, 1).\n",
        "- **Tanh:** Maps input values to the range (-1, 1).\n",
        "\n",
        "In PyTorch, these functions can be found in the `nn` module.\n",
        "\n",
        "### 6.2.3 Loss Functions\n",
        "\n",
        "Depending on the task (regression, classification, etc.), you'll need to choose an appropriate loss function:\n",
        "\n",
        "- **Mean Squared Error (MSE):** Commonly used for regression tasks.\n",
        "- **Cross-Entropy Loss:** Used for classification tasks.\n",
        "\n",
        "These can be accessed from `nn` as well.\n",
        "\n",
        "### 6.2.4 Optimizers\n",
        "\n",
        "Optimizers adjust the weights of the network based on the computed gradients. PyTorch provides a variety of optimization algorithms in the `torch.optim` module, with SGD (Stochastic Gradient Descent) and Adam being among the most popular.\n",
        "\n",
        "## 6.3 Training the MLP\n",
        "\n",
        "Training an MLP involves iterating over the dataset multiple times, feeding the data through the network, computing the loss, backpropagating the error, and updating the weights.\n",
        "\n",
        "```python\n",
        "import torch.optim as optim\n",
        "\n",
        "# Initialize the model, loss, and optimizer\n",
        "model = MLP(input_size=784, hidden_size=500, output_size=10)  # Example sizes for MNIST dataset\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop (simplified)\n",
        "for epoch in range(epochs):\n",
        "    for data, targets in dataloader:  # Assuming dataloader is an iterable over the dataset\n",
        "        optimizer.zero_grad()  # Reset gradients\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "```\n",
        "\n",
        "## 6.4 Evaluating the Model\n",
        "\n",
        "Once the model is trained, predictions can be made by simply passing data through the network:\n",
        "\n",
        "```python\n",
        "# Make predictions\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(test_data)\n",
        "    _, predicted = torch.max(test_outputs, 1)\n",
        "```\n",
        "\n",
        "## 6.5 Conclusion\n",
        "\n",
        "The Multilayer Perceptron serves as a stepping stone into the vast world of deep learning. By mastering the foundational concepts of MLPs, one gains the knowledge and confidence to delve into more complex architectures and applications. PyTorch, with its intuitive interface and powerful capabilities, offers an excellent platform to build, train, and evaluate these neural networks."
      ],
      "metadata": {
        "id": "NVVZ_hN5V_K0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 7: Your First Neural Network in PyTorch, Step by Step\n",
        "\n",
        "Building your first neural network can seem daunting, but with the right tools and a systematic approach, it becomes a straightforward task. In this chapter, we'll guide you step by step to build, train, and evaluate a simple neural network using PyTorch.\n",
        "\n",
        "## 7.1 Setting Up\n",
        "\n",
        "Ensure you have PyTorch installed. If not, you can install it via pip:\n",
        "\n",
        "```\n",
        "pip install torch torchvision\n",
        "```\n",
        "\n",
        "## 7.2 Dataset: MNIST\n",
        "\n",
        "For our first neural network, we'll use the MNIST dataset, a collection of handwritten digits. It's a commonly used dataset for introductory deep learning exercises.\n",
        "\n",
        "### 7.2.1 Loading the Dataset\n",
        "\n",
        "PyTorch provides utilities to load the MNIST dataset seamlessly:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Download and load training and test datasets\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32)\n",
        "```\n",
        "\n",
        "## 7.3 Building the Neural Network\n",
        "\n",
        "Let's build a simple feedforward neural network with one hidden layer:\n",
        "\n",
        "```python\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 500)  # Input layer: 28x28 pixels\n",
        "        self.fc2 = nn.Linear(500, 10)     # Output layer: 10 classes (digits 0-9)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)  # Flatten the input\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "```\n",
        "\n",
        "## 7.4 Training the Neural Network\n",
        "\n",
        "For training, we'll use the cross-entropy loss and the SGD optimizer:\n",
        "\n",
        "```python\n",
        "from torch import optim\n",
        "\n",
        "# Initialize the model, loss, and optimizer\n",
        "model = SimpleNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "epochs = 5\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for images, labels in train_loader:\n",
        "        # Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass\n",
        "        output = model(images)\n",
        "        \n",
        "        # Calculate loss\n",
        "        loss = criterion(output, labels)\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        \n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss.item():.4f}\")\n",
        "```\n",
        "\n",
        "## 7.5 Evaluating the Model\n",
        "\n",
        "After training, it's essential to evaluate the model's performance on unseen data:\n",
        "\n",
        "```python\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# No gradient computation during evaluation\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        output = model(images)\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Accuracy on test data: {100 * correct / total:.2f}%\")\n",
        "```\n",
        "\n",
        "## 7.6 Conclusion\n",
        "\n",
        "Congratulations! You've built, trained, and evaluated your first neural network using PyTorch. While this is a simple example, the foundational concepts remain consistent as you delve into more complex models and tasks. Remember that deep learning is as much an art as it is a science; experimenting with different architectures, hyperparameters, and techniques is crucial to obtaining optimal results. Armed with PyTorch and a curiosity to explore, the world of deep learning is yours to conquer."
      ],
      "metadata": {
        "id": "NOo_xpnnWSVw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 8: Creating a Training Loop for Your Models\n",
        "\n",
        "One of the core components of training a neural network is the training loop. It's where the magic happens: data is passed through the model, errors are computed, and weights are updated. This chapter will provide a detailed guide on creating an effective training loop in PyTorch.\n",
        "\n",
        "## 8.1 Basics of a Training Loop\n",
        "\n",
        "At its core, a training loop iterates over the dataset multiple times (epochs) and updates the model's weights to minimize the loss. Each iteration consists of:\n",
        "\n",
        "1. **Feedforward:** Compute the predicted output with the current weights.\n",
        "2. **Loss Calculation:** Compute the difference between the predicted output and the actual target values.\n",
        "3. **Backpropagation:** Compute the gradient of the loss with respect to model parameters.\n",
        "4. **Weight Update:** Adjust the weights using an optimization algorithm.\n",
        "\n",
        "## 8.2 The PyTorch Way\n",
        "\n",
        "### 8.2.1 Data Loaders\n",
        "\n",
        "PyTorch's `DataLoader` is a powerful utility that offers batch processing, shuffling, and parallel data loading.\n",
        "\n",
        "```python\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "```\n",
        "\n",
        "### 8.2.2 The Loop\n",
        "\n",
        "Here's a basic structure for the training loop:\n",
        "\n",
        "```python\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "        # Forward pass\n",
        "        outputs = model(data)\n",
        "        \n",
        "        # Compute loss\n",
        "        loss = criterion(outputs, targets)\n",
        "        \n",
        "        # Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        \n",
        "        # Weight update\n",
        "        optimizer.step()\n",
        "```\n",
        "\n",
        "## 8.3 Enhancing the Training Loop\n",
        "\n",
        "### 8.3.1 Monitoring the Training Progress\n",
        "\n",
        "To understand how well the training is progressing, it's helpful to monitor the loss, accuracy, and potentially other metrics.\n",
        "\n",
        "```python\n",
        "train_losses = []\n",
        "train_accuracy = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    correct = 0\n",
        "    total_loss = 0\n",
        "    \n",
        "    for data, targets in train_loader:\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, targets)\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        \n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = 100 * correct / len(train_dataset)\n",
        "\n",
        "    train_losses.append(avg_loss)\n",
        "    train_accuracy.append(accuracy)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
        "```\n",
        "\n",
        "### 8.3.2 Model Validation\n",
        "\n",
        "To ensure the model generalizes well, it's common to use a separate validation set.\n",
        "\n",
        "```python\n",
        "valid_losses = []\n",
        "valid_accuracy = []\n",
        "\n",
        "for data, targets in valid_loader:\n",
        "    outputs = model(data)\n",
        "    loss = criterion(outputs, targets)\n",
        "    \n",
        "    valid_losses.append(loss.item())\n",
        "    \n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    correct = (predicted == targets).sum().item()\n",
        "    valid_accuracy.append(100 * correct / len(targets))\n",
        "```\n",
        "\n",
        "### 8.3.3 Saving and Loading Models\n",
        "\n",
        "To reuse trained models, you can save and load their weights:\n",
        "\n",
        "```python\n",
        "# Save model\n",
        "torch.save(model.state_dict(), \"model.pth\")\n",
        "\n",
        "# Load model\n",
        "model = SimpleNN()\n",
        "model.load_state_dict(torch.load(\"model.pth\"))\n",
        "```\n",
        "\n",
        "## 8.4 Conclusion\n",
        "\n",
        "A well-constructed training loop is the backbone of any deep learning model training process. While the basic structure remains consistent, there are numerous enhancements, such as learning rate scheduling, early stopping, and gradient clipping, that can be integrated to improve training efficiency and model performance. With PyTorch's flexibility and comprehensive utilities, you have all the tools needed to create robust training loops tailored to your specific needs."
      ],
      "metadata": {
        "id": "hcRppFPqWkkV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 9: Evaluating PyTorch Models\n",
        "\n",
        "Once a model is trained, evaluating its performance is crucial. Proper evaluation provides insight into how well the model is likely to perform on unseen data. This chapter will guide you through various methods and metrics to evaluate PyTorch models effectively.\n",
        "\n",
        "## 9.1 The Importance of Evaluation\n",
        "\n",
        "Training a model is only half the battle. A model that performs well on training data might not necessarily do well on new data due to issues like overfitting. Thus, evaluating a model on a separate dataset (often called a validation or test set) gives a more realistic indication of its performance in real-world scenarios.\n",
        "\n",
        "## 9.2 Common Evaluation Metrics\n",
        "\n",
        "Depending on the problem type (regression, classification, etc.), various metrics can be used:\n",
        "\n",
        "### 9.2.1 Classification\n",
        "\n",
        "- **Accuracy:** The ratio of correctly predicted observations to the total observations.\n",
        "- **Confusion Matrix:** A table used to understand the performance of a classification model.\n",
        "- **Precision, Recall, and F1-score:** Metrics that provide more insight into the balance between true positive rate and positive predictive value.\n",
        "- **Area Under the ROC Curve (AUC-ROC):** Represents the model's ability to distinguish between classes.\n",
        "\n",
        "### 9.2.2 Regression\n",
        "\n",
        "- **Mean Absolute Error (MAE):** Represents the average of the absolute differences between predicted and actual values.\n",
        "- **Mean Squared Error (MSE):** The average of the squared differences between predicted and actual values.\n",
        "- **R-Squared:** Represents the proportion of variance for the dependent variable that's explained by independent variables in a regression model.\n",
        "\n",
        "## 9.3 Evaluating a Model in PyTorch\n",
        "\n",
        "### 9.3.1 Setting the Model to Evaluation Mode\n",
        "\n",
        "Before evaluation, ensure the model is in evaluation mode. This affects layers like dropout and batch normalization.\n",
        "\n",
        "```python\n",
        "model.eval()\n",
        "```\n",
        "\n",
        "### 9.3.2 Evaluation Loop\n",
        "\n",
        "The evaluation loop is similar to the training loop, but without weight updates.\n",
        "\n",
        "```python\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data, labels in test_loader:\n",
        "        outputs = model(data)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "```\n",
        "\n",
        "### 9.3.3 Using `torchmetrics`\n",
        "\n",
        "`torchmetrics` is a PyTorch library offering various evaluation metrics. It's designed to work seamlessly with PyTorch and provides efficient GPU-accelerated computations.\n",
        "\n",
        "```python\n",
        "from torchmetrics import Accuracy\n",
        "\n",
        "metric = Accuracy()\n",
        "accuracy = metric(predicted, labels)\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "```\n",
        "\n",
        "## 9.4 Advanced Evaluation Techniques\n",
        "\n",
        "### 9.4.1 K-Fold Cross-Validation\n",
        "\n",
        "Instead of having a static train/test split, data is divided into 'K' sets. The model is trained 'K' times, each time using a different set as the test set and the remaining sets as the training set. This method provides a more robust evaluation.\n",
        "\n",
        "### 9.4.2 Model Ensembling\n",
        "\n",
        "Multiple models' predictions are combined to produce a final prediction. Common techniques include:\n",
        "\n",
        "- **Voting:** Used for classification problems.\n",
        "- **Averaging:** Used for regression problems.\n",
        "- **Stacking:** Outputs of individual models become inputs for a \"meta\" model.\n",
        "\n",
        "## 9.5 Conclusion\n",
        "\n",
        "Evaluation is a critical step in the machine learning workflow. By properly evaluating a model, you ensure its readiness for deployment and gain insights into areas of improvement. Whether using basic metrics or advanced techniques, PyTorch provides the tools and flexibility necessary for robust and effective model evaluation. As with any tool, the key lies in understanding when and how to use these metrics to gain meaningful insights."
      ],
      "metadata": {
        "id": "klgG4LpHW4Uy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 10: Project: Building a Multiclass Classification Model in PyTorch\n",
        "\n",
        "In this project, we'll walk you through the process of building a multiclass classification model using PyTorch. We'll use the classic FashionMNIST dataset, which contains images of clothing items, as our dataset. The aim is to classify these items into one of ten classes.\n",
        "\n",
        "## 10.1 Setting the Stage\n",
        "\n",
        "### 10.1.1 Dataset Overview\n",
        "\n",
        "**FashionMNIST**:\n",
        "- Images: 28x28 grayscale images of 10 fashion categories.\n",
        "- Classes: T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot.\n",
        "\n",
        "### 10.1.2 Tools & Libraries\n",
        "\n",
        "Ensure you have PyTorch and torchvision installed.\n",
        "\n",
        "## 10.2 Data Loading and Preprocessing\n",
        "\n",
        "```python\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Data transformation\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Load FashionMNIST dataset\n",
        "train_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64)\n",
        "```\n",
        "\n",
        "## 10.3 Model Architecture\n",
        "\n",
        "We'll design a simple feedforward neural network with two hidden layers:\n",
        "\n",
        "```python\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MulticlassClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MulticlassClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "```\n",
        "\n",
        "## 10.4 Training the Model\n",
        "\n",
        "```python\n",
        "import torch.optim as optim\n",
        "\n",
        "# Instantiate model, loss, and optimizer\n",
        "model = MulticlassClassifier()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
        "        # Forward pass\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "```\n",
        "\n",
        "## 10.5 Model Evaluation\n",
        "\n",
        "```python\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for data, labels in test_loader:\n",
        "        outputs = model(data)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Accuracy of the model on the test images: {100 * correct / total:.2f}%\")\n",
        "```\n",
        "\n",
        "## 10.6 Conclusion & Next Steps\n",
        "\n",
        "Congratulations! You've successfully built a multiclass classification model using PyTorch. With this foundation, you can:\n",
        "\n",
        "- Experiment with more complex architectures, including CNNs.\n",
        "- Explore other datasets or real-world problems.\n",
        "- Integrate advanced techniques like data augmentation, regularization, and more.\n",
        "\n",
        "Remember, the journey in deep learning is iterative. Continually experiment, learn, and refine your models for better results."
      ],
      "metadata": {
        "id": "aKcODoWVbQe-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 11: Project: Building a Binary Classification Model in PyTorch\n",
        "\n",
        "Binary classification is one of the fundamental tasks in machine learning, where the goal is to categorize data into one of two classes. In this project, we will build a binary classification model using PyTorch. Our dataset will be a subset of the FashionMNIST dataset, where we aim to distinguish between two classes: 'Sandal' and 'Sneaker'.\n",
        "\n",
        "## 11.1 Setting the Stage\n",
        "\n",
        "### 11.1.1 Dataset Overview\n",
        "\n",
        "**FashionMNIST Subset**:\n",
        "- Images: 28x28 grayscale images.\n",
        "- Classes: Sandal, Sneaker.\n",
        "\n",
        "### 11.1.2 Tools & Libraries\n",
        "\n",
        "Ensure you have PyTorch and torchvision installed.\n",
        "\n",
        "## 11.2 Data Loading and Preprocessing\n",
        "\n",
        "First, we'll filter the dataset to retain only our classes of interest:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Data transformation\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Load and filter FashionMNIST dataset\n",
        "def filter_classes(label):\n",
        "    return label == 5 or label == 7\n",
        "\n",
        "train_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True, target_transform=filter_classes)\n",
        "test_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, target_transform=filter_classes)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64)\n",
        "```\n",
        "\n",
        "## 11.3 Model Architecture\n",
        "\n",
        "We'll design a simple feedforward neural network with one hidden layer:\n",
        "\n",
        "```python\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BinaryClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BinaryClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 512)\n",
        "        self.fc2 = nn.Linear(512, 1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = torch.sigmoid(self.fc2(x))\n",
        "        return x\n",
        "```\n",
        "\n",
        "## 11.4 Training the Model\n",
        "\n",
        "```python\n",
        "import torch.optim as optim\n",
        "\n",
        "# Instantiate model, loss, and optimizer\n",
        "model = BinaryClassifier()\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
        "        labels = labels.float().unsqueeze(1)  # Convert labels to float and adjust dimensions\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "```\n",
        "\n",
        "## 11.5 Model Evaluation\n",
        "\n",
        "```python\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for data, labels in test_loader:\n",
        "        labels = labels.float().unsqueeze(1)\n",
        "        outputs = model(data)\n",
        "        predicted = (outputs > 0.5).float()\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Accuracy of the model on the test images: {100 * correct / total:.2f}%\")\n",
        "```\n",
        "\n",
        "## 11.6 Conclusion & Next Steps\n",
        "\n",
        "Well done! You've built a binary classification model using PyTorch. With this foundational knowledge, you can:\n",
        "\n",
        "- Tackle more complex binary classification problems.\n",
        "- Apply techniques like data augmentation and regularization for better performance.\n",
        "- Dive into multiclass classification or other advanced architectures.\n",
        "\n",
        "As always in deep learning, iterative experimentation and learning are key. Keep refining and improving your models for ever-better results!"
      ],
      "metadata": {
        "id": "EgaFwWwDbjY2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 12: Project: Building a Regression Model in PyTorch\n",
        "\n",
        "Regression models aim to predict continuous values based on input features. In this project, we'll create a regression model using PyTorch to predict house prices based on various attributes. We'll be working with a simplified version of the famous Boston Housing dataset.\n",
        "\n",
        "## 12.1 Setting the Stage\n",
        "\n",
        "### 12.1.1 Dataset Overview\n",
        "\n",
        "**Boston Housing Dataset**:\n",
        "- Features: CRIM (crime rate), ZN (residential land zone), INDUS (non-retail business acres), CHAS (Charles River dummy variable), etc.\n",
        "- Target: Median value of owner-occupied homes (in $1000s).\n",
        "\n",
        "### 12.1.2 Tools & Libraries\n",
        "\n",
        "Ensure you have PyTorch installed. For this project, we'll also use the `sklearn` library to load and preprocess the data.\n",
        "\n",
        "## 12.2 Data Loading and Preprocessing\n",
        "\n",
        "First, let's load the dataset and normalize the features:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset\n",
        "data, target = datasets.load_boston(return_X_y=True)\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "data = scaler.fit_transform(data)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "train_data, test_data, train_target, test_target = train_test_split(data, target, test_size=0.2, random_state=42)\n",
        "\n",
        "train_data, test_data = torch.tensor(train_data, dtype=torch.float32), torch.tensor(test_data, dtype=torch.float32)\n",
        "train_target, test_target = torch.tensor(train_target, dtype=torch.float32).view(-1, 1), torch.tensor(test_target, dtype=torch.float32).view(-1, 1)\n",
        "```\n",
        "\n",
        "## 12.3 Model Architecture\n",
        "\n",
        "We'll design a simple feedforward neural network with two hidden layers:\n",
        "\n",
        "```python\n",
        "import torch.nn as nn\n",
        "\n",
        "class RegressionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RegressionModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(13, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, 1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = nn.ReLU()(self.fc1(x))\n",
        "        x = nn.ReLU()(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "```\n",
        "\n",
        "## 12.4 Training the Model\n",
        "\n",
        "```python\n",
        "import torch.optim as optim\n",
        "\n",
        "# Instantiate model, loss, and optimizer\n",
        "model = RegressionModel()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 200\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    outputs = model(train_data)\n",
        "    loss = criterion(outputs, train_target)\n",
        "    \n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "```\n",
        "\n",
        "## 12.5 Model Evaluation\n",
        "\n",
        "To evaluate the model, we'll compute the Mean Squared Error (MSE) on the test set:\n",
        "\n",
        "```python\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(test_data)\n",
        "    mse = criterion(test_outputs, test_target).item()\n",
        "\n",
        "print(f\"Mean Squared Error on the test set: {mse:.4f}\")\n",
        "```\n",
        "\n",
        "## 12.6 Conclusion & Next Steps\n",
        "\n",
        "Congratulations! You've built a regression model in PyTorch. With this foundation, you can:\n",
        "\n",
        "- Experiment with more complex architectures and datasets.\n",
        "- Apply regularization techniques to reduce overfitting.\n",
        "- Explore other loss functions or optimization algorithms.\n",
        "\n",
        "Deep learning for regression provides a flexible framework to capture intricate patterns in the data. As always, keep iterating and refining your models for even better performance!"
      ],
      "metadata": {
        "id": "YHZLWMysb0Gp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 13: Save and Load Your PyTorch Models\n",
        "\n",
        "In the deep learning workflow, after training a model, it's crucial to save it for future use, whether for inference, fine-tuning, or sharing. PyTorch provides intuitive methods to save and load models. This chapter delves into these methods and best practices for preserving your PyTorch models.\n",
        "\n",
        "## 13.1 Why Save Models?\n",
        "\n",
        "1. **Inference:** Once trained, models can be deployed in various applications to make predictions on new data.\n",
        "2. **Transfer Learning:** Trained models can serve as a starting point and be fine-tuned on a different task.\n",
        "3. **Archiving & Sharing:** Store models for future reference, or share with the community or colleagues.\n",
        "\n",
        "## 13.2 What to Save?\n",
        "\n",
        "When saving a model, you can opt to save:\n",
        "\n",
        "1. **Entire Model:** This includes both the model architecture and the trained parameters.\n",
        "2. **Model State Dict:** Only the trained parameters (recommended).\n",
        "3. **Optimizer State Dict:** Useful if you plan to resume training later.\n",
        "\n",
        "## 13.3 Saving and Loading Models\n",
        "\n",
        "### 13.3.1 Save Entire Model\n",
        "\n",
        "```python\n",
        "# Save\n",
        "torch.save(model, 'model_full.pth')\n",
        "\n",
        "# Load\n",
        "model = torch.load('model_full.pth')\n",
        "```\n",
        "\n",
        "**Note:** This method uses Python's `pickle` module and may lead to issues when loading the model on a different machine or platform.\n",
        "\n",
        "### 13.3.2 Save Model State Dict (Recommended)\n",
        "\n",
        "```python\n",
        "# Save\n",
        "torch.save(model.state_dict(), 'model_state_dict.pth')\n",
        "\n",
        "# Load\n",
        "model = SomeModelClass()  # You need to first initialize the original model class\n",
        "model.load_state_dict(torch.load('model_state_dict.pth'))\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "```\n",
        "\n",
        "### 13.3.3 Save Optimizer State\n",
        "\n",
        "If you're pausing training and plan to resume later, saving the optimizer's state is beneficial.\n",
        "\n",
        "```python\n",
        "# Save\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "}, 'checkpoint.pth')\n",
        "\n",
        "# Load\n",
        "checkpoint = torch.load('checkpoint.pth')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "```\n",
        "\n",
        "## 13.4 Best Practices\n",
        "\n",
        "1. **Always Save State Dicts:** They're more portable and don't carry unnecessary information.\n",
        "2. **Set `model.eval()`:** Before inference, always set the model to evaluation mode. It correctly configures layers like dropout and batch normalization.\n",
        "3. **Beware of Device Mismatches:** Models saved on one device (e.g., GPU) may not load directly on another (e.g., CPU). When loading, you can use the `map_location` argument to handle device mismatches.\n",
        "4. **Include Metadata:** When saving checkpoints, it's helpful to save other metadata like epoch number, latest loss value, etc., to have a comprehensive snapshot of the training state.\n",
        "\n",
        "## 13.5 Conclusion\n",
        "\n",
        "Saving and loading models is a fundamental skill in the deep learning workflow. PyTorch provides flexible tools to handle this task, making it easy to pause, resume, share, and deploy models. By understanding the underlying principles and best practices of model preservation, you ensure the integrity and reusability of your work, streamlining both development and deployment processes."
      ],
      "metadata": {
        "id": "D3j6uWqfXTFp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 14: Using Activation Functions in Deep Learning Models\n",
        "\n",
        "Activation functions play a pivotal role in deep learning models, introducing non-linearities that enable neural networks to learn complex patterns and relationships in data. This chapter delves into the importance, types, and application of activation functions in PyTorch.\n",
        "\n",
        "## 14.1 Why Activation Functions?\n",
        "\n",
        "Linear transformations, such as matrix multiplications and additions, are inherently limited. No matter how many linear layers are stacked in a neural network, their combined transformation remains linear. Activation functions introduce non-linearities, enabling neural networks to become universal function approximators.\n",
        "\n",
        "## 14.2 Types of Activation Functions\n",
        "\n",
        "### 14.2.1 Sigmoid\n",
        "\n",
        "The sigmoid function maps input values to the range (0, 1). It's historically popular but less used in deep networks due to vanishing gradient issues.\n",
        "\n",
        "\\[ \\sigma(x) = \\frac{1}{1 + \\exp(-x)} \\]\n",
        "\n",
        "### 14.2.2 Hyperbolic Tangent (tanh)\n",
        "\n",
        "Similar to sigmoid but maps input values to the range (-1, 1). It centers the output around zero, which can make learning easier in subsequent layers.\n",
        "\n",
        "\\[ \\tanh(x) = \\frac{2}{1 + \\exp(-2x)} - 1 \\]\n",
        "\n",
        "### 14.2.3 Rectified Linear Unit (ReLU)\n",
        "\n",
        "A simple yet powerful function that returns the input for positive values and zero for negative values. It has become a standard activation function for many types of neural networks due to its efficacy and computational efficiency.\n",
        "\n",
        "\\[ \\text{ReLU}(x) = \\max(0, x) \\]\n",
        "\n",
        "### 14.2.4 Leaky ReLU\n",
        "\n",
        "A variant of ReLU that allows a small gradient for negative values, mitigating the \"dying ReLU\" problem where neurons can sometimes get stuck during training.\n",
        "\n",
        "\\[ \\text{Leaky ReLU}(x) = \\max(\\alpha x, x) \\]\n",
        "\n",
        "where \\( \\alpha \\) is a small constant.\n",
        "\n",
        "### 14.2.5 Softmax\n",
        "\n",
        "Often used in the output layer of a network for multi-class classification problems. It converts raw scores (logits) into probabilities by exponentiating them and then normalizing.\n",
        "\n",
        "\\[ \\text{Softmax}(x)_i = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)} \\]\n",
        "\n",
        "## 14.3 Activation Functions in PyTorch\n",
        "\n",
        "PyTorch provides built-in support for various activation functions in the `torch.nn.functional` module:\n",
        "\n",
        "```python\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Sample tensor\n",
        "x = torch.tensor([-1.0, 0.0, 1.0])\n",
        "\n",
        "# Activation functions\n",
        "sigmoid_output = F.sigmoid(x)\n",
        "tanh_output = F.tanh(x)\n",
        "relu_output = F.relu(x)\n",
        "leaky_relu_output = F.leaky_relu(x, negative_slope=0.01)\n",
        "```\n",
        "\n",
        "For the softmax function:\n",
        "\n",
        "```python\n",
        "logits = torch.tensor([2.0, 1.0, 0.1])\n",
        "softmax_output = F.softmax(logits, dim=0)\n",
        "```\n",
        "\n",
        "## 14.4 Choosing the Right Activation Function\n",
        "\n",
        "1. **Default Choice:** ReLU is a good default choice for hidden layers in most deep learning models.\n",
        "2. **Avoiding Dead Neurons:** If you observe a significant portion of your neurons dying (i.e., always outputting zero), consider using variants like Leaky ReLU or Parametric ReLU.\n",
        "3. **Output Layer:** For binary classification, use sigmoid. For multi-class classification, use softmax.\n",
        "\n",
        "## 14.5 Conclusion\n",
        "\n",
        "Activation functions are the heartbeats of neural networks, instilling them with the capacity to learn complex representations. While the choice of activation function can influence model performance, often the architecture, data, and other factors play a more substantial role. As with many aspects of deep learning, experimentation is key. Familiarizing yourself with the available options and understanding their nuances will empower you to make informed decisions in your deep learning journey."
      ],
      "metadata": {
        "id": "DIcdQMaKXif_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 15: Loss Functions in PyTorch Models\n",
        "\n",
        "Loss functions, or cost functions, are a foundational element in training deep learning models. They quantify how well a model's predictions match the actual data, guiding the optimization process. In this chapter, we'll delve into the various loss functions available in PyTorch and their applications.\n",
        "\n",
        "## 15.1 Understanding Loss Functions\n",
        "\n",
        "A loss function measures the difference between the actual and predicted values. The goal during training is to minimize this loss, enabling the model to make better predictions.\n",
        "\n",
        "## 15.2 Common Loss Functions in PyTorch\n",
        "\n",
        "### 15.2.1 Mean Squared Error (MSE) Loss\n",
        "\n",
        "Used primarily for regression tasks.\n",
        "\n",
        "\\[ \\text{MSE}(y, \\hat{y}) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\]\n",
        "\n",
        "In PyTorch:\n",
        "\n",
        "```python\n",
        "loss = torch.nn.MSELoss()\n",
        "```\n",
        "\n",
        "### 15.2.2 Cross-Entropy Loss\n",
        "\n",
        "Used for classification tasks. It quantifies the difference between two probability distributions.\n",
        "\n",
        "\\[ \\text{CE}(y, \\hat{y}) = -\\sum_{i} y_i \\log(\\hat{y}_i) \\]\n",
        "\n",
        "In PyTorch:\n",
        "\n",
        "```python\n",
        "loss = torch.nn.CrossEntropyLoss()\n",
        "```\n",
        "\n",
        "### 15.2.3 Binary Cross-Entropy Loss\n",
        "\n",
        "Used for binary classification tasks.\n",
        "\n",
        "\\[ \\text{BCE}(y, \\hat{y}) = -\\frac{1}{n} \\sum_{i=1}^{n} y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\]\n",
        "\n",
        "In PyTorch:\n",
        "\n",
        "```python\n",
        "loss = torch.nn.BCELoss()\n",
        "```\n",
        "\n",
        "### 15.2.4 L1 Loss\n",
        "\n",
        "Measures the mean absolute difference between the actual and predicted values. It's less sensitive to outliers compared to MSE.\n",
        "\n",
        "\\[ \\text{L1}(y, \\hat{y}) = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i| \\]\n",
        "\n",
        "In PyTorch:\n",
        "\n",
        "```python\n",
        "loss = torch.nn.L1Loss()\n",
        "```\n",
        "\n",
        "### 15.2.5 Negative Log Likelihood (NLL) Loss\n",
        "\n",
        "Often used in combination with a softmax layer in multi-class classification problems.\n",
        "\n",
        "In PyTorch:\n",
        "\n",
        "```python\n",
        "loss = torch.nn.NLLLoss()\n",
        "```\n",
        "\n",
        "## 15.3 Using Loss Functions in PyTorch\n",
        "\n",
        "Once you've chosen a loss function, you can compute the loss between your model's predictions and the actual data. Here's a simple example using Cross-Entropy Loss:\n",
        "\n",
        "```python\n",
        "import torch.nn as nn\n",
        "\n",
        "# Sample data\n",
        "outputs = torch.tensor([[0.1, 0.2, 0.7], [0.5, 0.2, 0.3]])  # Raw logits from the model for two samples\n",
        "labels = torch.tensor([2, 0])  # Actual labels\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "loss = criterion(outputs, labels)\n",
        "```\n",
        "\n",
        "## 15.4 Choosing the Right Loss Function\n",
        "\n",
        "1. **Task Type:** The choice often depends on the task. Use MSE for regression and cross-entropy for classification.\n",
        "2. **Model Stability:** If training is unstable, consider using a variant or combination of loss functions.\n",
        "3. **Data Characteristics:** For data with many outliers, L1 loss might be more appropriate than MSE.\n",
        "\n",
        "## 15.5 Conclusion\n",
        "\n",
        "Loss functions play a pivotal role in guiding the training of deep learning models. They offer a quantifiable metric that the optimization algorithm, like gradient descent, uses to adjust model parameters. Understanding the nuances of different loss functions and when to apply them can significantly impact the effectiveness and efficiency of the training process. With PyTorch's comprehensive suite of loss functions, you're well-equipped to tackle a wide array of deep learning challenges."
      ],
      "metadata": {
        "id": "VBJ1hyhsX2y1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 16: Using Dropout Regularization in PyTorch Models\n",
        "\n",
        "Deep learning models, with their large number of parameters, can easily overfit to the training data. Regularization techniques help prevent this by constraining the model's capacity. One popular regularization technique is **dropout**. This chapter will explore the dropout technique and its implementation in PyTorch.\n",
        "\n",
        "## 16.1 Understanding Dropout\n",
        "\n",
        "Dropout is a regularization method wherein, during training, random subsets of neurons are \"dropped out\" or temporarily deactivated along with their associated connections. This prevents any single neuron from becoming overly specialized and promotes distributed representations.\n",
        "\n",
        "Key points:\n",
        "1. Dropout is applied only during training.\n",
        "2. During inference (or evaluation), no neurons are dropped out. Instead, their outputs are scaled to account for the dropout applied during training.\n",
        "\n",
        "## 16.2 Why Use Dropout?\n",
        "\n",
        "1. **Prevent Overfitting:** By deactivating random neurons during training, dropout prevents the model from becoming overly reliant on any specific neuron, reducing overfitting.\n",
        "2. **Ensemble Effect:** Dropout can be thought of as training a pseudo-ensemble of neural networks, with each training iteration using a different \"thinned\" version of the network.\n",
        "3. **Improved Convergence:** Sometimes, dropout can make the optimization landscape smoother, facilitating faster convergence.\n",
        "\n",
        "## 16.3 Dropout in PyTorch\n",
        "\n",
        "PyTorch makes it straightforward to integrate dropout into your models.\n",
        "\n",
        "### 16.3.1 Using Dropout in Models\n",
        "\n",
        "Here's how to use dropout in a simple neural network:\n",
        "\n",
        "```python\n",
        "import torch.nn as nn\n",
        "\n",
        "class DropoutModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DropoutModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 500)\n",
        "        self.fc2 = nn.Linear(500, 250)\n",
        "        self.fc3 = nn.Linear(250, 10)\n",
        "        self.dropout = nn.Dropout(0.5)  # 50% dropout\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)  # Apply dropout after activation function\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "```\n",
        "\n",
        "### 16.3.2 Dropout During Evaluation\n",
        "\n",
        "When evaluating the model, dropout should be turned off to ensure all neurons are active. You can do this by setting the model to evaluation mode:\n",
        "\n",
        "```python\n",
        "model.eval()\n",
        "```\n",
        "\n",
        "When you want to switch back to training mode (and activate dropout again), you can use:\n",
        "\n",
        "```python\n",
        "model.train()\n",
        "```\n",
        "\n",
        "## 16.4 Variants of Dropout\n",
        "\n",
        "Several variants and improvements over the standard dropout technique exist:\n",
        "\n",
        "1. **Spatial Dropout:** Used in convolutional networks, it drops entire 1D/2D feature maps instead of individual elements.\n",
        "2. **Alpha Dropout:** Maintains the mean and variance of inputs to be closer to that of original inputs, suitable for SELU activation.\n",
        "3. **Variational Dropout:** Maintains the same dropout mask for all time steps in recurrent neural networks.\n",
        "\n",
        "## 16.5 Conclusion\n",
        "\n",
        "Dropout is a powerful regularization technique that can significantly improve a model's generalization, especially in scenarios with limited data or large networks. Its intuitive concept of \"thinning\" the network during training and easy implementation in frameworks like PyTorch makes it a valuable tool in the deep learning practitioner's arsenal. By understanding when and how to use dropout effectively, you can train more robust and reliable neural network models."
      ],
      "metadata": {
        "id": "NRiXER3qYGiB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 17: Using Learning Rate Scheduling in PyTorch Training\n",
        "\n",
        "The learning rate is one of the most critical hyperparameters in training deep learning models. While a constant learning rate can be effective, often, dynamically adjusting the learning rate during training can lead to faster convergence and improved generalization. This chapter explores the concept of learning rate scheduling and its implementation in PyTorch.\n",
        "\n",
        "## 17.1 Importance of Learning Rate\n",
        "\n",
        "The learning rate controls the step size when updating model parameters during training. If set too high, training may diverge; if set too low, training may be slow or get stuck in local minima.\n",
        "\n",
        "## 17.2 Why Use Learning Rate Scheduling?\n",
        "\n",
        "1. **Faster Convergence:** Starting with a larger learning rate and reducing it can lead to quicker convergence.\n",
        "2. **Better Generalization:** Some schedules can help the model generalize better by introducing an \"annealing\" effect.\n",
        "3. **Avoid Local Minima:** By adjusting the learning rate, the model can potentially escape local minima or saddle points.\n",
        "\n",
        "## 17.3 Common Learning Rate Schedules\n",
        "\n",
        "### 17.3.1 Step Decay\n",
        "\n",
        "Reduces the learning rate by a factor after a specified number of epochs.\n",
        "\n",
        "### 17.3.2 Exponential Decay\n",
        "\n",
        "Reduces the learning rate exponentially after each epoch.\n",
        "\n",
        "### 17.3.3 Cosine Annealing\n",
        "\n",
        "Adjusts the learning rate using a cosine function, leading to large changes in the beginning and smaller adjustments towards the end.\n",
        "\n",
        "### 17.3.4 One-Cycle Learning Rate\n",
        "\n",
        "Starts by increasing the learning rate and then decreases it. This policy is particularly popular in training some types of neural networks.\n",
        "\n",
        "### 17.3.5 Reduce on Plateau\n",
        "\n",
        "Reduces the learning rate when a metric (e.g., validation loss) has stopped improving.\n",
        "\n",
        "## 17.4 Learning Rate Scheduling in PyTorch\n",
        "\n",
        "PyTorch provides the `torch.optim.lr_scheduler` module, which offers a variety of learning rate schedules.\n",
        "\n",
        "### 17.4.1 Implementing Step Decay\n",
        "\n",
        "```python\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "```\n",
        "\n",
        "### 17.4.2 Implementing Reduce on Plateau\n",
        "\n",
        "```python\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
        "```\n",
        "\n",
        "In the training loop, you'd use `scheduler.step(val_loss)` where `val_loss` is your validation loss.\n",
        "\n",
        "## 17.5 Using Schedulers in Training\n",
        "\n",
        "After defining a scheduler, it needs to be called during the training loop:\n",
        "\n",
        "```python\n",
        "for epoch in range(num_epochs):\n",
        "    # Training code...\n",
        "\n",
        "    # Update the learning rate\n",
        "    scheduler.step()\n",
        "```\n",
        "\n",
        "## 17.6 Conclusion\n",
        "\n",
        "Learning rate scheduling offers a dynamic approach to adjusting the learning rate during training, leading to potential improvements in convergence speed and model generalization. By understanding the various scheduling strategies and how to implement them in PyTorch, you can experiment with and tailor your training process to achieve optimal results."
      ],
      "metadata": {
        "id": "URtKDyQIYVPm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 18: Training a PyTorch Model with DataLoader and Dataset\n",
        "\n",
        "In deep learning, handling and processing large datasets efficiently is crucial. PyTorch provides a comprehensive ecosystem for data loading with its `Dataset` and `DataLoader` classes. This chapter explores these classes and demonstrates how to use them in training PyTorch models.\n",
        "\n",
        "## 18.1 Understanding Dataset and DataLoader\n",
        "\n",
        "### 18.1.1 Dataset\n",
        "\n",
        "`Dataset` is a PyTorch class for representing datasets. It provides two main methods:\n",
        "\n",
        "1. `__len__`: Returns the size of the dataset.\n",
        "2. `__getitem__`: Allows the dataset to be indexed, so it can work like a list (`dataset[i]`).\n",
        "\n",
        "### 18.1.2 DataLoader\n",
        "\n",
        "`DataLoader` wraps a `Dataset` and provides mini-batches of data, making it easier to iterate over datasets during training. It also offers features like data shuffling and parallel data loading.\n",
        "\n",
        "## 18.2 Using Built-in Datasets\n",
        "\n",
        "PyTorch's `torchvision` library offers built-in datasets like MNIST, CIFAR10, and ImageNet.\n",
        "\n",
        "```python\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Data transformation\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Load MNIST dataset\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32)\n",
        "```\n",
        "\n",
        "## 18.3 Creating Custom Datasets\n",
        "\n",
        "For custom datasets, you can subclass the `Dataset` class and implement the `__len__` and `__getitem__` methods:\n",
        "\n",
        "```python\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]\n",
        "```\n",
        "\n",
        "## 18.4 Training with DataLoader\n",
        "\n",
        "Once you have a `DataLoader`, you can easily iterate over batches of data in your training loop:\n",
        "\n",
        "```python\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
        "        # Forward pass, loss computation, backward pass, optimizer step...\n",
        "```\n",
        "\n",
        "## 18.5 Benefits of DataLoader\n",
        "\n",
        "1. **Efficiency:** DataLoader loads data in parallel, utilizing multi-core CPUs efficiently.\n",
        "2. **Flexibility:** Easily switch between different datasets and mini-batch sizes.\n",
        "3. **Features:** Built-in support for data batching, shuffling, and more.\n",
        "\n",
        "## 18.6 Conclusion\n",
        "\n",
        "Handling data is a foundational aspect of deep learning. PyTorch's `Dataset` and `DataLoader` classes provide a flexible and efficient system for managing and iterating over data, whether it's a standard dataset like CIFAR10 or custom data. By understanding and leveraging these utilities, you can streamline the data handling process, ensuring that it complements and enhances the model training experience."
      ],
      "metadata": {
        "id": "K7ctTgW3Ykis"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 19: Using PyTorch Deep Learning Models with scikit-learn\n",
        "\n",
        "Scikit-learn is one of the most popular libraries for machine learning in Python. While it is renowned for its vast array of traditional machine learning algorithms and tools, it doesn't directly support deep learning models like those built with PyTorch. However, there's often a need to integrate PyTorch models with scikit-learn workflows, especially for tasks like cross-validation, grid search, etc.\n",
        "\n",
        "This chapter will explore how to bridge the gap between PyTorch and scikit-learn, making it possible to use deep learning models seamlessly within scikit-learn pipelines.\n",
        "\n",
        "## 19.1 Why Integrate PyTorch with scikit-learn?\n",
        "\n",
        "1. **Leverage Established Workflows:** Scikit-learn provides tools like `train_test_split`, `GridSearchCV`, and `Pipeline` which can be useful even for deep learning models.\n",
        "2. **Hybrid Models:** Combining traditional machine learning models with deep learning components.\n",
        "3. **Model Evaluation:** Use scikit-learn's extensive metrics and evaluation utilities.\n",
        "\n",
        "## 19.2 Building a PyTorch Estimator for scikit-learn\n",
        "\n",
        "Scikit-learn uses the estimator API, where models implement `fit`, `predict`, and optionally `transform` methods. To use a PyTorch model with scikit-learn, you need to wrap it in such an estimator.\n",
        "\n",
        "### 19.2.1 Basic Wrapper\n",
        "\n",
        "Here's a basic example of wrapping a PyTorch model for a binary classification task:\n",
        "\n",
        "```python\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "import torch\n",
        "\n",
        "class PyTorchClassifier(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, model, criterion, optimizer, epochs):\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        dataset = torch.utils.data.TensorDataset(X, y)\n",
        "        loader = torch.utils.data.DataLoader(dataset, batch_size=32)\n",
        "        \n",
        "        for epoch in range(self.epochs):\n",
        "            for data, labels in loader:\n",
        "                outputs = self.model(data)\n",
        "                loss = self.criterion(outputs, labels)\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(X)\n",
        "        return torch.argmax(outputs, dim=1)\n",
        "```\n",
        "\n",
        "## 19.3 Using the PyTorch Estimator in scikit-learn\n",
        "\n",
        "With the wrapper, you can now use the PyTorch model within scikit-learn workflows:\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "clf = PyTorchClassifier(model, criterion, optimizer, epochs=10)\n",
        "clf.fit(X_train, y_train)\n",
        "predictions = clf.predict(X_test)\n",
        "```\n",
        "\n",
        "You can also use other scikit-learn utilities:\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Evaluation\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "\n",
        "# Hyperparameter tuning\n",
        "param_grid = {'epochs': [5, 10, 15], 'optimizer__lr': [0.01, 0.001]}\n",
        "grid_search = GridSearchCV(clf, param_grid, cv=3)\n",
        "grid_search.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "## 19.4 Considerations\n",
        "\n",
        "1. **Data Conversion:** Scikit-learn typically works with NumPy arrays, but PyTorch uses tensors. Ensure proper conversions.\n",
        "2. **Performance:** While scikit-learn's utilities are convenient, they might not be optimized for deep learning tasks. Be cautious about performance bottlenecks.\n",
        "3. **Complexity:** This integration is more suitable for simpler models. Complex deep learning workflows might not fit neatly into scikit-learn's paradigm.\n",
        "\n",
        "## 19.5 Conclusion\n",
        "\n",
        "Bridging PyTorch and scikit-learn allows data scientists and ML practitioners to tap into the best of both worlds. Whether it's leveraging scikit-learn's powerful utilities or integrating deep learning components into traditional ML pipelines, this hybrid approach can offer flexibility and efficiency. By understanding the nuances and potential pitfalls of this integration, you can harness the combined power of these libraries effectively."
      ],
      "metadata": {
        "id": "ZKLWirZ9Y5dF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 20: Optimize Hyperparameters with Grid Search\n",
        "\n",
        "Training a deep learning model involves multiple hyperparameters that influence model performance. Choosing the right set of hyperparameters can significantly affect the model's accuracy and convergence speed. Grid search is a systematic method to search for the best hyperparameters from a predefined grid of values. This chapter delves into hyperparameter optimization using grid search.\n",
        "\n",
        "## 20.1 What are Hyperparameters?\n",
        "\n",
        "Hyperparameters are parameters whose values are set before training a model. They aren't updated during training. Common hyperparameters in deep learning include:\n",
        "\n",
        "1. Learning rate\n",
        "2. Batch size\n",
        "3. Number of epochs\n",
        "4. Dropout rate\n",
        "5. Number of layers/neurons in neural networks\n",
        "\n",
        "## 20.2 Understanding Grid Search\n",
        "\n",
        "Grid search is a brute-force method where you define a grid of hyperparameter values and evaluate the model performance for each combination. The combination yielding the best performance is chosen.\n",
        "\n",
        "For instance, given hyperparameters `learning_rate` with values [0.001, 0.01, 0.1] and `batch_size` with values [32, 64, 128], grid search will evaluate the model for all \\(3 \\times 3 = 9\\) combinations.\n",
        "\n",
        "## 20.3 Implementing Grid Search with scikit-learn\n",
        "\n",
        "The `GridSearchCV` class in scikit-learn can be utilized, even with PyTorch models (as covered in the previous chapter).\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Assuming you have a PyTorchClassifier wrapper as before\n",
        "clf = PyTorchClassifier(model, criterion, optimizer, epochs=10)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'epochs': [5, 10],\n",
        "    'optimizer__lr': [0.001, 0.01],\n",
        "    'batch_size': [32, 64]\n",
        "}\n",
        "\n",
        "# Initialize GridSearch\n",
        "grid_search = GridSearchCV(clf, param_grid, cv=3, scoring='accuracy')\n",
        "\n",
        "# Fit data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "best_params = grid_search.best_params_\n",
        "```\n",
        "\n",
        "## 20.4 Benefits and Limitations\n",
        "\n",
        "### Benefits:\n",
        "\n",
        "1. **Systematic Search:** Grid search ensures every combination is tried, ensuring no potential configuration is missed.\n",
        "2. **Parallelization:** Many grid search implementations, including in scikit-learn, support parallel evaluations.\n",
        "\n",
        "### Limitations:\n",
        "\n",
        "1. **Computationally Expensive:** As the number of hyperparameters and their potential values grows, the number of evaluations can grow exponentially.\n",
        "2. **Fixed Grid:** Only evaluates model performance at specific points, potentially missing optimal values between grid points.\n",
        "\n",
        "## 20.5 Alternatives to Grid Search\n",
        "\n",
        "1. **Random Search:** Instead of evaluating all combinations, random combinations of hyperparameters are chosen and evaluated. Often more efficient than grid search.\n",
        "2. **Bayesian Optimization:** Uses probabilistic models to predict which hyperparameters might yield better results, focusing the search in promising regions.\n",
        "3. **Genetic Algorithms:** Inspired by the process of natural selection, these algorithms evolve sets of hyperparameters over iterations.\n",
        "\n",
        "## 20.6 Conclusion\n",
        "\n",
        "Hyperparameter tuning is a critical step in the machine learning workflow. While grid search offers a straightforward and systematic approach, it may not always be the most efficient, especially for high-dimensional hyperparameter spaces. By understanding the principles, benefits, and limitations of grid search, along with knowledge of alternative methods, you can make informed decisions on optimizing hyperparameters for your models effectively."
      ],
      "metadata": {
        "id": "DyyPEUthZJC5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 21: Managing Training Process with Checkpoints and Early Stopping\n",
        "\n",
        "Deep learning models often require prolonged training times. Managing this process efficiently can help save time, computational resources, and ensure that the best model is retrieved. Techniques like checkpoints and early stopping are essential tools in this endeavor. This chapter delves into these techniques and their implementation in PyTorch.\n",
        "\n",
        "## 21.1 Importance of Checkpoints\n",
        "\n",
        "Training deep learning models, especially on large datasets, can be time-consuming. If training is interrupted due to issues like system crashes or resource constraints, all progress could be lost. Checkpointing periodically saves the model's state, ensuring that you can resume from the last checkpoint rather than starting over.\n",
        "\n",
        "**Advantages of Checkpoints:**\n",
        "1. **Resilience:** Resume training from interruptions.\n",
        "2. **Analysis:** Examine model states at different stages of training.\n",
        "3. **Flexibility:** Experiment with different strategies without retraining from scratch.\n",
        "\n",
        "## 21.2 Implementing Checkpoints in PyTorch\n",
        "\n",
        "PyTorch provides a simple mechanism to save and load model states:\n",
        "\n",
        "```python\n",
        "# Save checkpoint\n",
        "torch.save({\n",
        "    'epoch': epoch,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'loss': loss,\n",
        "}, 'checkpoint.pth')\n",
        "\n",
        "# Load checkpoint\n",
        "checkpoint = torch.load('checkpoint.pth')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "start_epoch = checkpoint['epoch']\n",
        "loss = checkpoint['loss']\n",
        "```\n",
        "\n",
        "## 21.3 Early Stopping\n",
        "\n",
        "Early stopping halts training when a monitored metric (e.g., validation loss) stops improving. This not only saves time but also prevents overfitting, as models can start to overfit if trained for too many epochs.\n",
        "\n",
        "### 21.3.1 Implementing Early Stopping\n",
        "\n",
        "A simple early stopping mechanism can be implemented by tracking the best value of the monitored metric and stopping training if it doesn't improve for a specified number of epochs (often called \"patience\").\n",
        "\n",
        "```python\n",
        "best_val_loss = float('inf')\n",
        "patience = 10\n",
        "epochs_without_improvement = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training and validation code...\n",
        "    \n",
        "    # Check for improvement\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        epochs_without_improvement = 0\n",
        "    else:\n",
        "        epochs_without_improvement += 1\n",
        "        \n",
        "    if epochs_without_improvement == patience:\n",
        "        print(\"Stopping early!\")\n",
        "        break\n",
        "```\n",
        "\n",
        "## 21.4 Combining Checkpoints and Early Stopping\n",
        "\n",
        "You can combine both techniques to periodically save model checkpoints and also halt training early if no improvements are observed:\n",
        "\n",
        "1. At the end of each epoch, save a checkpoint.\n",
        "2. Monitor a validation metric.\n",
        "3. If the metric doesn't improve for a set number of epochs, stop training and revert the model to the best checkpoint.\n",
        "\n",
        "## 21.5 Conclusion\n",
        "\n",
        "Managing the training process is crucial to efficiently utilize computational resources and retrieve the best possible model. Checkpoints ensure resilience against interruptions, while early stopping helps focus computational efforts and prevent overfitting. By incorporating these strategies into your training loop, you can achieve better results with less hassle and in less time."
      ],
      "metadata": {
        "id": "QgXQrxz1ZYRz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 22: Visualizing a PyTorch Model\n",
        "\n",
        "Visualization plays an essential role in understanding, interpreting, and debugging deep learning models. This chapter will explore various methods to visualize PyTorch models, from their architecture to their internal activations.\n",
        "\n",
        "## 22.1 Why Visualization?\n",
        "\n",
        "1. **Understand Model Architecture:** Get an overview of the layers, shapes, and connections.\n",
        "2. **Debugging:** Identify issues in the model's structure or data flow.\n",
        "3. **Interpretability:** Understand what the model has learned and how it makes decisions.\n",
        "4. **Educational:** Helps in teaching and explaining neural network concepts.\n",
        "\n",
        "## 22.2 Visualizing Model Architecture\n",
        "\n",
        "### 22.2.1 Using `torchsummary`\n",
        "\n",
        "The `torchsummary` library provides a Keras-style `summary` method for PyTorch models, detailing layers, output shapes, and parameters.\n",
        "\n",
        "```python\n",
        "from torchsummary import summary\n",
        "\n",
        "model = ...  # Some PyTorch model\n",
        "summary(model, input_size=(channels, H, W))\n",
        "```\n",
        "\n",
        "### 22.2.2 Using TensorBoard\n",
        "\n",
        "PyTorch has native support for TensorBoard, a visualization tool from TensorFlow.\n",
        "\n",
        "```python\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "writer = SummaryWriter()\n",
        "writer.add_graph(model, input_data)\n",
        "writer.close()\n",
        "```\n",
        "\n",
        "You can then view the model architecture using the TensorBoard interface.\n",
        "\n",
        "## 22.3 Visualizing Activations and Feature Maps\n",
        "\n",
        "Visualizing intermediate activations can offer insights into what the model \"sees\" at various stages.\n",
        "\n",
        "```python\n",
        "# Hook to capture activations\n",
        "activations = {}\n",
        "\n",
        "def hook_fn(module, input, output):\n",
        "    activations[name] = output\n",
        "\n",
        "# Register hook for a specific layer\n",
        "layer_name = 'conv1'\n",
        "getattr(model, layer_name).register_forward_hook(hook_fn)\n",
        "\n",
        "# Forward pass to capture activations\n",
        "output = model(input_data)\n",
        "\n",
        "# Visualize the captured activations\n",
        "activation = activations['conv1']\n",
        "# ... Visualization code ...\n",
        "```\n",
        "\n",
        "## 22.4 Visualizing Weights and Gradients\n",
        "\n",
        "Similar to activations, visualizing weights and gradients can help in debugging and understanding the training process.\n",
        "\n",
        "```python\n",
        "# Weights of a specific layer\n",
        "weights = model.conv1.weight.data\n",
        "\n",
        "# Gradients of a specific layer (after backward pass)\n",
        "gradients = model.conv1.weight.grad\n",
        "```\n",
        "\n",
        "## 22.5 Visualizing with Integrated Tools\n",
        "\n",
        "### 22.5.1 Netron\n",
        "\n",
        "[Netron](https://github.com/lutzroeder/netron) is a standalone tool that provides a visual representation of various deep learning models, including PyTorch models. You can save the model and open it with Netron:\n",
        "\n",
        "```python\n",
        "torch.save(model.state_dict(), 'model.pth')\n",
        "```\n",
        "\n",
        "Then open `model.pth` using Netron to visualize.\n",
        "\n",
        "### 22.5.2 TensorBoard's Embedding Projector\n",
        "\n",
        "For visualizing embeddings, TensorBoard offers an embedding projector that provides 2D or 3D visualizations.\n",
        "\n",
        "```python\n",
        "writer = SummaryWriter()\n",
        "writer.add_embedding(embeddings, metadata=labels)\n",
        "writer.close()\n",
        "```\n",
        "\n",
        "## 22.6 Conclusion\n",
        "\n",
        "Visualization is a powerful tool in the deep learning toolkit. While PyTorch provides the necessary building blocks, several external tools and libraries further simplify and enhance the visualization process. By effectively visualizing model architectures, activations, weights, and embeddings, you gain deeper insights into your models, leading to better design, debugging, and interpretation decisions."
      ],
      "metadata": {
        "id": "j_llm3KnZqlt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 23: Understanding Model Behavior During Training by Visualizing Metrics\n",
        "\n",
        "Monitoring metrics during training is essential to understand the behavior of a model, diagnose issues, and ensure optimal performance. Visualizing these metrics provides a clear picture of how the model is progressing. This chapter will delve into the importance of these visualizations and how to effectively use them with PyTorch.\n",
        "\n",
        "## 23.1 Importance of Monitoring Metrics\n",
        "\n",
        "1. **Evaluate Convergence:** Visualization helps determine if the model is converging and when it has converged.\n",
        "2. **Detect Overfitting:** A growing gap between training and validation metrics can indicate overfitting.\n",
        "3. **Hyperparameter Tuning:** Metrics help in evaluating the effect of different hyperparameters.\n",
        "4. **Diagnose Issues:** Stagnant or erratic metrics can indicate problems like vanishing/exploding gradients or inappropriate learning rates.\n",
        "\n",
        "## 23.2 Common Metrics to Monitor\n",
        "\n",
        "1. **Loss:** The most fundamental metric, indicating how well the model's predictions match the actual data.\n",
        "2. **Accuracy:** In classification tasks, it measures the proportion of correctly classified instances.\n",
        "3. **Learning Rate:** Especially if using adaptive learning rate methods.\n",
        "4. **Gradient Norms:** Can help diagnose vanishing/exploding gradient issues.\n",
        "\n",
        "## 23.3 Visualizing Metrics with TensorBoard\n",
        "\n",
        "TensorBoard is a versatile tool for visualizing metrics during training. PyTorch provides native support for TensorBoard through `torch.utils.tensorboard`.\n",
        "\n",
        "### 23.3.1 Setting up TensorBoard with PyTorch\n",
        "\n",
        "```python\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "writer = SummaryWriter(log_dir='./logs')\n",
        "```\n",
        "\n",
        "### 23.3.2 Logging Metrics\n",
        "\n",
        "After each epoch or iteration, you can log metrics:\n",
        "\n",
        "```python\n",
        "for epoch in range(num_epochs):\n",
        "    # Training and validation code...\n",
        "    \n",
        "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
        "    writer.add_scalar('Loss/val', val_loss, epoch)\n",
        "    writer.add_scalar('Accuracy/train', train_acc, epoch)\n",
        "    writer.add_scalar('Accuracy/val', val_acc, epoch)\n",
        "```\n",
        "\n",
        "### 23.3.3 Viewing in TensorBoard\n",
        "\n",
        "Once metrics are logged, run TensorBoard pointing to the log directory:\n",
        "\n",
        "```bash\n",
        "tensorboard --logdir=./logs\n",
        "```\n",
        "\n",
        "You can then access TensorBoard in a browser, providing a dynamic interface to visualize and analyze the metrics.\n",
        "\n",
        "## 23.4 Visualizing Metrics with Other Tools\n",
        "\n",
        "While TensorBoard is powerful, there are other tools like Weights & Biases, Neptune, and Comet.ml that offer similar functionality, often with additional features. They can integrate with PyTorch and provide platforms to monitor, analyze, and share training metrics.\n",
        "\n",
        "## 23.5 Tips for Effective Metric Visualization\n",
        "\n",
        "1. **Smoothing:** Training metrics can be noisy. Smooth curves can make trends more apparent.\n",
        "2. **Multiple Runs:** When tuning hyperparameters or experimenting, overlaying metrics from multiple runs helps in comparison.\n",
        "3. **Use Histograms:** For metrics like weight distributions or gradient norms, histograms can be more informative than scalar plots.\n",
        "4. **Monitor Early Indicators:** Instead of waiting for the final evaluation metrics, monitor indicators that might give early insights into potential issues or the final performance.\n",
        "\n",
        "## 23.6 Conclusion\n",
        "\n",
        "Visualizing training metrics is crucial for a clear and effective deep learning workflow. It provides insights into the model's behavior, helps diagnose issues, and guides hyperparameter tuning. By leveraging tools like TensorBoard and following best practices, you can ensure a thorough understanding of your models throughout the training process, leading to better and more robust outcomes."
      ],
      "metadata": {
        "id": "HUc-u2A2Z5h6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 24: From MLP to CNN and RNN\n",
        "\n",
        "Deep learning boasts a variety of neural network architectures, each tailored to specific types of data and tasks. While the Multilayer Perceptron (MLP) serves as the foundational building block, more complex architectures like Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN) have proven pivotal in handling image and sequence data, respectively. This chapter offers an introduction to these architectures and their roles.\n",
        "\n",
        "## 24.1 Multilayer Perceptron (MLP)\n",
        "\n",
        "**Overview:**  \n",
        "MLP, also known as a feedforward neural network, consists of multiple layers of nodes in a directed graph. Each layer fully connects to the next layer.\n",
        "\n",
        "**Applications:**  \n",
        "- Basic classification tasks\n",
        "- Regression tasks\n",
        "\n",
        "**Limitations:**  \n",
        "- Doesn't capture spatial hierarchies in image data effectively\n",
        "- Doesn't handle sequential data with temporal dependencies\n",
        "\n",
        "## 24.2 Convolutional Neural Networks (CNN)\n",
        "\n",
        "**Overview:**  \n",
        "CNNs are specifically designed for processing grid-like data such as images. They use convolutional layers that apply convolution operations, capturing spatial hierarchies in the data.\n",
        "\n",
        "**Key Components:**  \n",
        "- **Convolutional Layers:** Extract features using small, learnable filters.\n",
        "- **Pooling Layers:** Reduce spatial dimensions while retaining important information.\n",
        "- **Fully Connected Layers:** Classify based on the features extracted.\n",
        "\n",
        "**Applications:**  \n",
        "- Image classification\n",
        "- Object detection\n",
        "- Image generation\n",
        "\n",
        "**Advantages over MLP:**  \n",
        "- Reduced number of parameters due to shared weights in convolutional layers.\n",
        "- Better handling of spatial hierarchies in image data.\n",
        "\n",
        "## 24.3 Recurrent Neural Networks (RNN)\n",
        "\n",
        "**Overview:**  \n",
        "RNNs are designed for sequence data, where order matters. They maintain a hidden state that captures information from previous steps in the sequence.\n",
        "\n",
        "**Key Components:**  \n",
        "- **Hidden State:** Retains information from previous time steps.\n",
        "- **Recurrent Layers:** Process each element of the sequence while considering the hidden state.\n",
        "\n",
        "**Applications:**  \n",
        "- Natural language processing tasks (e.g., machine translation, sentiment analysis)\n",
        "- Time series forecasting\n",
        "- Music generation\n",
        "\n",
        "**Advantages over MLP:**  \n",
        "- Captures temporal dependencies in sequence data.\n",
        "- Can handle sequences of variable lengths.\n",
        "\n",
        "**Limitations:**  \n",
        "- Difficulty in capturing long-range dependencies due to vanishing gradient problem.\n",
        "- Sequential processing can be slower compared to feedforward networks.\n",
        "\n",
        "## 24.4 LSTM and GRU: Advanced RNNs\n",
        "\n",
        "To combat the vanishing gradient problem in basic RNNs:\n",
        "\n",
        "- **Long Short-Term Memory (LSTM):** Introduces three gates (input, forget, and output) to control information flow, allowing the model to learn long-term dependencies.\n",
        "- **Gated Recurrent Units (GRU):** A simplified version of LSTM with two gates, often faster and requiring fewer parameters.\n",
        "\n",
        "## 24.5 Conclusion\n",
        "\n",
        "While MLPs offer a foundational understanding of neural networks, specific data types and tasks necessitate specialized architectures. CNNs revolutionized image processing by leveraging spatial hierarchies, while RNNs, especially their advanced variants, made significant strides in processing sequence data. Understanding the strengths and applications of each architecture ensures the selection of the right tool for the task at hand, optimizing performance and efficiency."
      ],
      "metadata": {
        "id": "g8TUC3dBaLhe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 25: Building a Convolutional Neural Network in PyTorch\n",
        "\n",
        "Convolutional Neural Networks (CNNs) have fundamentally transformed the field of computer vision. In this chapter, we'll guide you through building a basic CNN using PyTorch, one of the most popular deep learning frameworks.\n",
        "\n",
        "## 25.1 Understanding the Basics of CNN\n",
        "\n",
        "Before diving into the code, it's essential to understand the primary components of a CNN:\n",
        "\n",
        "1. **Convolutional Layers:** Extract features from input data using filters/kernels.\n",
        "2. **Pooling Layers:** Reduce the spatial dimensions of the feature maps.\n",
        "3. **Fully Connected Layers:** Perform classification based on the extracted features.\n",
        "\n",
        "## 25.2 Setting Up\n",
        "\n",
        "First, ensure you have PyTorch and the necessary libraries:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "```\n",
        "\n",
        "## 25.3 Building the CNN Model\n",
        "\n",
        "Here's a simple CNN for image classification:\n",
        "\n",
        "```python\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        \n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        \n",
        "        # Pooling layer\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        \n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(32 * 7 * 7, 512)\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # First convolutional layer, followed by ReLU and pooling\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        \n",
        "        # Second convolutional layer, followed by ReLU and pooling\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        \n",
        "        # Flatten the feature maps\n",
        "        x = x.view(-1, 32 * 7 * 7)\n",
        "        \n",
        "        # First fully connected layer\n",
        "        x = F.relu(self.fc1(x))\n",
        "        \n",
        "        # Second fully connected layer (output layer)\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        return x\n",
        "```\n",
        "\n",
        "This architecture assumes input images of size \\(28 \\times 28\\), grayscale (hence `in_channels=1`), and aims to classify them into one of ten classes.\n",
        "\n",
        "## 25.4 Training the CNN\n",
        "\n",
        "To train the CNN, you'll need a dataset, a loss function, and an optimizer. For this example, let's assume you're working with the MNIST dataset:\n",
        "\n",
        "```python\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Data loaders\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "train_loader = torch.utils.data.DataLoader(datasets.MNIST('./data', train=True, download=True, transform=transform), batch_size=64, shuffle=True)\n",
        "\n",
        "# Model, Loss, and Optimizer\n",
        "model = SimpleCNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "        # Forward pass\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, targets)\n",
        "        \n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "```\n",
        "\n",
        "## 25.5 Conclusion\n",
        "\n",
        "Building a CNN in PyTorch is straightforward thanks to its modular and intuitive design. The example provided is a basic introduction, but CNNs can become much more complex and sophisticated. Understanding the foundational concepts and knowing how to implement them in PyTorch provides a solid basis for diving deeper into the world of convolutional neural networks and computer vision."
      ],
      "metadata": {
        "id": "FPa6QRiBajj_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 26: Handwritten Digit Recognition with LeNet-5 Model in PyTorch\n",
        "\n",
        "In this chapter, we'll dive into a classic computer vision problem using one of the pioneering convolutional neural network architectures - LeNet-5. Designed by Yann LeCun in 1998, LeNet-5 was developed for handwritten and machine-printed character recognition. We'll apply it to the well-known MNIST dataset containing images of handwritten digits.\n",
        "\n",
        "## 26.1 Setting the Stage\n",
        "\n",
        "### 26.1.1 Dataset Overview\n",
        "\n",
        "**MNIST**:\n",
        "- Images: 28x28 grayscale images of handwritten digits.\n",
        "- Classes: 10 (0 through 9).\n",
        "\n",
        "### 26.1.2 Tools & Libraries\n",
        "\n",
        "Ensure you have PyTorch and torchvision installed.\n",
        "\n",
        "## 26.2 Data Loading and Preprocessing\n",
        "\n",
        "```python\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Data transformation: Resizing is required as original LeNet-5 architecture accepts 32x32 images.\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Load MNIST dataset\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64)\n",
        "```\n",
        "\n",
        "## 26.3 LeNet-5 Model Architecture\n",
        "\n",
        "```python\n",
        "import torch.nn as nn\n",
        "\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1)\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(16*5*5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = nn.ReLU()(self.conv1(x))\n",
        "        x = nn.MaxPool2d(kernel_size=2)(x)\n",
        "        x = nn.ReLU()(self.conv2(x))\n",
        "        x = nn.MaxPool2d(kernel_size=2)(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = nn.ReLU()(self.fc1(x))\n",
        "        x = nn.ReLU()(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "```\n",
        "\n",
        "## 26.4 Training the Model\n",
        "\n",
        "```python\n",
        "import torch.optim as optim\n",
        "\n",
        "# Instantiate model, loss, and optimizer\n",
        "model = LeNet5()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
        "        # Forward pass\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "```\n",
        "\n",
        "## 26.5 Model Evaluation\n",
        "\n",
        "```python\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for data, labels in test_loader:\n",
        "        outputs = model(data)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Accuracy of the model on the test images: {100 * correct / total:.2f}%\")\n",
        "```\n",
        "\n",
        "## 26.6 Conclusion & Future Exploration\n",
        "\n",
        "You've successfully built and trained the LeNet-5 model on the MNIST dataset using PyTorch! While LeNet-5 is relatively simple compared to modern architectures, it serves as an essential foundation in the history of CNNs.\n",
        "\n",
        "For future exploration:\n",
        "- Try out more recent architectures like AlexNet, VGG, or ResNet on more complex datasets.\n",
        "- Introduce techniques like data augmentation to further improve model performance.\n",
        "- Explore deeper architectures and see how depth impacts performance.\n",
        "\n",
        "By understanding and implementing historical architectures like LeNet-5, you gain insights into the evolution and principles of deep learning, positioning you well for tackling more advanced challenges."
      ],
      "metadata": {
        "id": "m5SsqwRwcHL_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 27: LSTM for Time Series Prediction in PyTorch\n",
        "\n",
        "Long Short-Term Memory (LSTM) networks, a type of recurrent neural network (RNN), have proven to be particularly effective at handling sequences, such as time series data. In this chapter, we will guide you through the process of using an LSTM to make predictions on time series data using PyTorch.\n",
        "\n",
        "## 27.1 Setting the Stage\n",
        "\n",
        "### 27.1.1 Data Overview\n",
        "\n",
        "For this project, we'll use a hypothetical dataset representing monthly sales of a product over several years. Our goal is to predict future sales based on past data.\n",
        "\n",
        "## 27.2 Data Loading and Preprocessing\n",
        "\n",
        "Let's start by generating some synthetic time series data and preprocessing it:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "time = np.arange(120, dtype=\"float32\")\n",
        "sales = np.sin(time / 12) + np.sin(time / 8) * 0.5 + 0.5 * np.random.randn(120)\n",
        "\n",
        "# Normalize data\n",
        "max_val = max(sales)\n",
        "min_val = min(sales)\n",
        "sales_normalized = (sales - min_val) / (max_val - min_val)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "sales_tensor = torch.FloatTensor(sales_normalized).view(-1)\n",
        "\n",
        "# Plot data\n",
        "plt.plot(time, sales, label=\"Sales over time\")\n",
        "plt.xlabel(\"Month\")\n",
        "plt.ylabel(\"Sales\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "## 27.3 Creating Sequences\n",
        "\n",
        "To train our LSTM, we'll convert our time series data into overlapping sequences:\n",
        "\n",
        "```python\n",
        "def create_sequences(data, seq_length):\n",
        "    sequences = []\n",
        "    target = []\n",
        "\n",
        "    for i in range(len(data) - seq_length):\n",
        "        seq = data[i:i+seq_length]\n",
        "        label = data[i+seq_length:i+seq_length+1]\n",
        "        sequences.append(seq)\n",
        "        target.append(label)\n",
        "\n",
        "    return torch.stack(sequences), torch.stack(target)\n",
        "\n",
        "seq_length = 12\n",
        "sequences, labels = create_sequences(sales_tensor, seq_length)\n",
        "```\n",
        "\n",
        "## 27.4 LSTM Model Architecture\n",
        "\n",
        "```python\n",
        "import torch.nn as nn\n",
        "\n",
        "class TimeSeriesLSTM(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_layer_size=50, output_size=1):\n",
        "        super(TimeSeriesLSTM, self).__init__()\n",
        "        self.hidden_layer_size = hidden_layer_size\n",
        "        self.lstm = nn.LSTM(input_size, hidden_layer_size)\n",
        "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
        "        self.hidden_cell = (torch.zeros(1, 1, self.hidden_layer_size),\n",
        "                            torch.zeros(1, 1, self.hidden_layer_size))\n",
        "\n",
        "    def forward(self, input_seq):\n",
        "        lstm_out, self.hidden_cell = self.lstm(input_seq.view(len(input_seq), 1, -1), self.hidden_cell)\n",
        "        predictions = self.linear(lstm_out.view(len(input_seq), -1))\n",
        "        return predictions\n",
        "```\n",
        "\n",
        "## 27.5 Training the Model\n",
        "\n",
        "```python\n",
        "model = TimeSeriesLSTM()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 150\n",
        "for i in range(epochs):\n",
        "    for seq, label in zip(sequences, labels):\n",
        "        optimizer.zero_grad()\n",
        "        model.hidden_cell = (torch.zeros(1, 1, model.hidden_layer_size),\n",
        "                             torch.zeros(1, 1, model.hidden_layer_size))\n",
        "\n",
        "        y_pred = model(seq)\n",
        "\n",
        "        loss = criterion(y_pred, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if i%25 == 0:\n",
        "        print(f\"Epoch {i} loss: {loss.item()}\")\n",
        "```\n",
        "\n",
        "## 27.6 Model Evaluation and Prediction\n",
        "\n",
        "Here, you'd evaluate the model's performance on a test set (if available) and make future predictions.\n",
        "\n",
        "## 27.7 Conclusion & Next Steps\n",
        "\n",
        "You've built an LSTM model in PyTorch for time series prediction! LSTMs are a powerful tool for various sequential tasks. For further exploration:\n",
        "\n",
        "- Try more complex datasets or real-world time series data.\n",
        "- Experiment with more layers or bidirectional LSTMs.\n",
        "- Explore other RNN architectures like GRU.\n",
        "\n",
        "Understanding LSTMs and their applications in time series forecasting provides a solid foundation for exploring more advanced topics in deep learning and sequence modeling."
      ],
      "metadata": {
        "id": "MdbxkXP0cL6E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 28: Text Generation with LSTM in PyTorch\n",
        "\n",
        "Recurrent Neural Networks (RNNs), particularly Long Short-Term Memory (LSTM) networks, have demonstrated remarkable success in a variety of sequence-based tasks. One exciting application of LSTMs is text generation. In this chapter, we'll guide you through building an LSTM-based model in PyTorch to generate text.\n",
        "\n",
        "## 28.1 Setting the Stage\n",
        "\n",
        "### 28.1.1 Data Overview\n",
        "\n",
        "For this project, we'll use a subset of text from a classic book (e.g., \"Alice's Adventures in Wonderland\"). Our goal is to train the LSTM to generate new text in the style of the book.\n",
        "\n",
        "## 28.2 Data Loading and Preprocessing\n",
        "\n",
        "Start by loading the text data and performing basic preprocessing:\n",
        "\n",
        "```python\n",
        "with open(\"path_to_text_file.txt\", 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Lowercase the text and remove any special characters\n",
        "text = ''.join([c for c in text if c.isalnum() or c.isspace()]).lower()\n",
        "\n",
        "# Create a dictionary to map characters to integers and vice versa\n",
        "chars = sorted(list(set(text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "```\n",
        "\n",
        "## 28.3 Creating Sequences\n",
        "\n",
        "To train the LSTM, convert the text into overlapping sequences of characters:\n",
        "\n",
        "```python\n",
        "seq_length = 100\n",
        "sequences = []\n",
        "next_chars = []\n",
        "\n",
        "for i in range(0, len(text) - seq_length, 1):\n",
        "    seq = text[i:i + seq_length]\n",
        "    next_char = text[i + seq_length]\n",
        "    sequences.append([char_to_int[char] for char in seq])\n",
        "    next_chars.append(char_to_int[next_char])\n",
        "\n",
        "X = torch.tensor(sequences, dtype=torch.float32)\n",
        "Y = torch.tensor(next_chars)\n",
        "```\n",
        "\n",
        "## 28.4 LSTM Model Architecture\n",
        "\n",
        "```python\n",
        "class TextGeneratorLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers):\n",
        "        super(TextGeneratorLSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        y_pred = self.linear(lstm_out[:, -1])\n",
        "        return y_pred\n",
        "```\n",
        "\n",
        "## 28.5 Training the Model\n",
        "\n",
        "```python\n",
        "model = TextGeneratorLSTM(input_dim=len(chars), hidden_dim=256, output_dim=len(chars), n_layers=2)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 30\n",
        "for epoch in range(epochs):\n",
        "    for batch_seq, batch_next_char in zip(X, Y):\n",
        "        optimizer.zero_grad()\n",
        "        seq_onehot = nn.functional.one_hot(batch_seq, num_classes=len(chars)).float()\n",
        "        y_pred = model(seq_onehot.unsqueeze(0))\n",
        "        \n",
        "        loss = criterion(y_pred, batch_next_char.unsqueeze(0))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
        "```\n",
        "\n",
        "## 28.6 Text Generation\n",
        "\n",
        "After training, the model can generate new text:\n",
        "\n",
        "```python\n",
        "def generate_text(start_string, generate_length=100):\n",
        "    input_sequence = torch.tensor([char_to_int[c] for c in start_string], dtype=torch.long)\n",
        "    generated_text = start_string\n",
        "\n",
        "    for i in range(generate_length):\n",
        "        input_onehot = nn.functional.one_hot(input_sequence, num_classes=len(chars)).float()\n",
        "        y_pred = model(input_onehot.unsqueeze(0))\n",
        "        predicted_char = int_to_char[torch.argmax(y_pred, dim=2)[0].item()]\n",
        "        generated_text += predicted_char\n",
        "        input_sequence = torch.cat([input_sequence, torch.tensor([char_to_int[predicted_char]], dtype=torch.long)], dim=0)\n",
        "        input_sequence = input_sequence[1:]\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "print(generate_text(\"alice\", 200))\n",
        "```\n",
        "\n",
        "## 28.7 Conclusion & Next Steps\n",
        "\n",
        "Congratulations! You've built a text-generating LSTM model. Here's what you can explore further:\n",
        "\n",
        "- Use a larger corpus for more varied and coherent text generation.\n",
        "- Experiment with different architectures, including GRUs or deeper LSTMs.\n",
        "- Implement techniques like teacher forcing or gradient clipping to improve the model's performance.\n",
        "\n",
        "By understanding the basics of text generation using LSTMs, you're poised to explore more advanced topics in deep learning and natural language processing."
      ],
      "metadata": {
        "id": "INs-k-YocqHY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 30: LLMs (Language Learning Models)\n",
        "\n",
        "Language Learning Models (LLMs) are an evolution in the world of Natural Language Processing (NLP), encompassing a wide range of models designed to understand, generate, and interact using human language. In this chapter, we'll delve into the key concepts of LLMs, their architectures, and their applications.\n",
        "\n",
        "## 30.1 What are LLMs?\n",
        "\n",
        "Language Learning Models are advanced machine learning models trained on vast amounts of text data. They aim to capture the nuances, grammar, context, and semantics of language, making them adept at a myriad of language-based tasks without task-specific training data.\n",
        "\n",
        "## 30.2 Evolution of LLMs\n",
        "\n",
        "The journey of LLMs began with simpler models and has evolved over time:\n",
        "\n",
        "1. **RNNs (Recurrent Neural Networks)**: Can process sequences by maintaining an internal state.\n",
        "2. **LSTMs (Long Short-Term Memory)**: An evolution over RNNs, better at handling long-term dependencies.\n",
        "3. **GRUs (Gated Recurrent Units)**: A variation of LSTMs, with a simpler structure.\n",
        "4. **Transformers**: Introduced the concept of attention, allowing the model to focus on specific parts of the input.\n",
        "5. **BERT (Bidirectional Encoder Representations from Transformers)**: Trained to predict masked words in a sentence, capturing bidirectional context.\n",
        "6. **GPT (Generative Pre-trained Transformer)**: Uses transformers for generative tasks.\n",
        "7. **Advanced LLMs**: Variants and improvements on the above models, such as GPT-2, GPT-3, T5, etc., trained on vast datasets, exhibiting human-like text generation capabilities.\n",
        "\n",
        "## 30.3 Key Components\n",
        "\n",
        "1. **Attention Mechanism**: Allows models to focus on specific parts of the input sequence, capturing dependencies regardless of the distance between elements.\n",
        "2. **Embeddings**: Convert words/tokens into vectors, capturing semantic meanings.\n",
        "3. **Positional Encodings**: In transformer architectures, since they don't have a sense of order inherently, positional encodings give a sense of position to the model.\n",
        "\n",
        "## 30.4 Training LLMs\n",
        "\n",
        "Training advanced LLMs requires:\n",
        "- Vast datasets: Billions of words.\n",
        "- Significant computational power: Multiple GPUs or TPUs.\n",
        "- Regularization techniques: To prevent overfitting, given the model's massive parameter count.\n",
        "\n",
        "## 30.5 Applications\n",
        "\n",
        "1. **Text Generation**: Generate coherent and contextually relevant text.\n",
        "2. **Translation**: Translate text from one language to another.\n",
        "3. **Question Answering**: Extract answers from provided content.\n",
        "4. **Summarization**: Produce concise summaries of longer texts.\n",
        "5. **Classification**: Categorize texts into predefined classes.\n",
        "6. **And more**: Virtually any NLP task can benefit from LLMs.\n",
        "\n",
        "## 30.6 Challenges and Considerations\n",
        "\n",
        "1. **Computational Requirements**: Training LLMs from scratch requires significant resources.\n",
        "2. **Fine-tuning**: While pre-trained LLMs are available, they often need to be fine-tuned for specific tasks.\n",
        "3. **Ethical Concerns**: LLMs can generate misleading or harmful content, and biases in training data can lead to biased model outputs.\n",
        "4. **Interpretability**: LLMs, given their complexity, are challenging to interpret.\n",
        "\n",
        "## 30.7 Conclusion & Future Outlook\n",
        "\n",
        "Language Learning Models represent the cutting edge in NLP. As research progresses, these models are expected to become even more efficient, versatile, and accessible. The fusion of LLMs with other domains, such as vision (in models like CLIP) or reinforcement learning, promises exciting advancements in AI."
      ],
      "metadata": {
        "id": "sBe0ZUrme8mF"
      }
    }
  ]
}